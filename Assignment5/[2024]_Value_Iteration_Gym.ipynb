{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YczE55EXiYx9",
        "outputId": "420f17aa-05f4-4326-f2ad-20b4fd3c40c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gymnasium in c:\\users\\legion 5 pro\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\legion 5 pro\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\legion 5 pro\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium) (3.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\legion 5 pro\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium) (4.11.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\legion 5 pro\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yhSyhfEy4XSD"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import time\n",
        "from IPython import display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kHf1dAVKAcZm"
      },
      "outputs": [],
      "source": [
        "env = gym.make('FrozenLake-v1', render_mode=\"ansi\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-6usoQHAmqh",
        "outputId": "c37e6f40-9d01-4f50-cce9-99de3a7315fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(0.3333333333333333, 1, 0.0, False),\n",
              " (0.3333333333333333, 0, 0.0, False),\n",
              " (0.3333333333333333, 0, 0.0, False)]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env.unwrapped.P[0][3] # Transition model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wh7Su0h0AqQz",
        "outputId": "2a5b804e-5b7a-4a84-8ff9-09ee97790ca0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env.observation_space.n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZ68w5bpBScC",
        "outputId": "05c0a7b0-d868-46ec-e3f3-8ecf85eb1d24"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env.action_space.n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VWLnvY7VBvIZ"
      },
      "outputs": [],
      "source": [
        "def play(env, policy, render=False):\n",
        "    state, _ = env.reset()\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = policy[state]\n",
        "        next_state, reward, done, info, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "        if render:\n",
        "            print(env.render())\n",
        "            time.sleep(0.5)\n",
        "            if not done:\n",
        "                display.clear_output(wait=True)\n",
        "        state = next_state\n",
        "\n",
        "    return (total_reward, steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcuDDx6rC5YE",
        "outputId": "9707ead1-097f-4517-b9ce-8cddda663a7f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.0, 12)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "policy_0 = np.asarray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
        "play(env, policy_0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJ1CJNPhDGPA",
        "outputId": "abfd77fb-c766-454c-e0d5-a774fee2f2d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.0, 8)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "policy_0 = np.asarray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
        "play(env, policy_0, True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdyjjtGZC9NX",
        "outputId": "a7ae95f8-3060-4c4d-93d9-5fcfd4ce1b55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "\n",
            "0.0 8\n"
          ]
        }
      ],
      "source": [
        "reward, step = 0, None\n",
        "policy_1 = np.asarray([0, 1, 1, 3, 1, 0, 2, 0, 1, 1, 2, 2, 3, 3, 1, 0])\n",
        "reward, step = play(env, policy_1, True)\n",
        "print(reward, step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tt0VhyMuDasc",
        "outputId": "9c362b93-1aaf-4a84-eb16-d63f58f6265b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.0, 4)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "policy_2 = np.array([1, 1, 1, 3, 0, 1, 2, 3, 1, 1, 2, 3, 2, 2, 1, 3])\n",
        "play(env, policy_2, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hp6qhRFJDxWR",
        "outputId": "66da48db-c2b7-448f-b33c-5055f5f3bd30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(1.0, 21)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "policy_3 = np.array([0, 3, 0, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0])\n",
        "play(env, policy_3, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "JU8Q1qMxD6Po"
      },
      "outputs": [],
      "source": [
        "def play_multiple_times(env, policy, max_episodes):\n",
        "    success = 0\n",
        "    list_of_steps = []\n",
        "    for i in range(max_episodes):\n",
        "        total_reward, steps = play(env, policy)\n",
        "\n",
        "        if total_reward > 0:\n",
        "            success += 1\n",
        "            list_of_steps.append(steps)\n",
        "\n",
        "    # print(f'Number of successes: {success}/{max_episodes}')\n",
        "    # print(f'Average number of steps: {np.mean(list_of_steps)}')\n",
        "    return (success, np.mean(list_of_steps))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G427z17PEmjQ",
        "outputId": "f20e5e7b-19f8-4a98-b0a2-8e85220594a0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Legion 5 Pro\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "c:\\Users\\Legion 5 Pro\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0, nan)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "policy_0 = np.asarray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
        "play_multiple_times(env, policy_0, 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1bkhaFdDmj_",
        "outputId": "02e8420d-466e-4035-9e8f-f61bdf391779"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(51, 11.137254901960784)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "policy_1 = np.asarray([0, 1, 1, 3, 1, 0, 2, 0, 1, 1, 2, 2, 3, 3, 1, 0])\n",
        "play_multiple_times(env, policy_1, 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZYhsb_VEtuR",
        "outputId": "933da714-6efe-453c-f9a5-570feaba301a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(106, 14.971698113207546)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "policy_2 = np.array([1, 1, 1, 3, 0, 1, 2, 3, 1, 1, 2, 3, 2, 2, 1, 3])\n",
        "play_multiple_times(env, policy_2, 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvvHdMesEzTH",
        "outputId": "3c21ebe0-7c87-4b0b-ed65-350bf2cd5e1c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(777, 43.88030888030888)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "policy_3 = np.array([0, 3, 0, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0])\n",
        "play_multiple_times(env, policy_3, 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "epsilon = 1e-9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "bSomNpxJE5lP"
      },
      "outputs": [],
      "source": [
        "def policy_evaluation(env, policy, max_iters=500, gamma=0.9, bool_print = True):\n",
        "    # Initialize the values of all states to be 0\n",
        "    v_values = np.zeros(env.observation_space.n)\n",
        "\n",
        "    for i in range(max_iters):\n",
        "        prev_v_values = np.copy(v_values)\n",
        "\n",
        "        # Update the value of each state\n",
        "        for state in range(env.observation_space.n):\n",
        "            action = policy[state]\n",
        "\n",
        "            # Compute the q-value of the action\n",
        "            q_value = 0\n",
        "            for prob, next_state, reward, done in env.unwrapped.P[state][action]:\n",
        "                q_value += prob * (reward + gamma * prev_v_values[next_state])\n",
        "\n",
        "            v_values[state] = q_value # update v-value\n",
        "\n",
        "        # Check convergence\n",
        "        if np.all(np.isclose(v_values, prev_v_values)):\n",
        "            if bool_print:\n",
        "                print(f'Converged at {i}-th iteration.')\n",
        "            break\n",
        "\n",
        "    return v_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7IhqEOgGkQX",
        "outputId": "d8f855b6-cfff-45d4-a433-3a76e84e3e55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converged at 0-th iteration.\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "policy_0 = np.asarray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
        "v_values_0 = policy_evaluation(env, policy_0)\n",
        "print(v_values_0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMjJKI3GGrsN",
        "outputId": "a934f69c-a70b-4e06-f1dc-25eafbeb0c47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converged at 48-th iteration.\n",
            "[0.01904157 0.01519815 0.03161906 0.02371389 0.02538879 0.\n",
            " 0.06648515 0.         0.05924054 0.13822794 0.18999823 0.\n",
            " 0.         0.21152109 0.56684236 0.        ]\n"
          ]
        }
      ],
      "source": [
        "policy_1 = np.asarray([0, 1, 1, 3, 1, 0, 2, 0, 1, 1, 2, 2, 3, 3, 1, 0])\n",
        "v_values_1 = policy_evaluation(env, policy_1)\n",
        "print(v_values_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-26M77nEfcV",
        "outputId": "a49842f4-7901-4576-c52d-5fb414de4f1e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.all(v_values_1 >= v_values_0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l49O1N8QG0S2",
        "outputId": "f6e8d433-9d58-4a46-d5e9-ad7746a33acb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converged at 53-th iteration.\n",
            "[0.02889625 0.01951972 0.03616977 0.0271268  0.04790519 0.\n",
            " 0.07391985 0.         0.08288277 0.19339319 0.21022995 0.\n",
            " 0.         0.35153135 0.62684674 0.        ]\n"
          ]
        }
      ],
      "source": [
        "policy_2 = np.array([1, 1, 1, 3, 0, 1, 2, 3, 1, 1, 2, 3, 2, 2, 1, 3])\n",
        "v_values_2 = policy_evaluation(env, policy_2)\n",
        "print(v_values_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22pRvreGE3Yt",
        "outputId": "07915473-ca14-4a0c-e7c7-027c6adf0498"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.all(v_values_2 >= v_values_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTYYFq6BEXDd",
        "outputId": "12fd0fd0-24d6-48dd-a456-ed9323f50d3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converged at 80-th iteration.\n",
            "[0.06888666 0.06141097 0.07440714 0.05580443 0.09185068 0.\n",
            " 0.11220679 0.         0.14543323 0.24749485 0.29961611 0.\n",
            " 0.         0.37993438 0.63901935 0.        ]\n"
          ]
        }
      ],
      "source": [
        "policy_3 = np.array([0, 3, 0, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0])\n",
        "v_values_3 = policy_evaluation(env, policy_3)\n",
        "print(v_values_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcEfU3NYE7xN",
        "outputId": "78187f65-f5d4-4827-fbd3-3ca0feb9ccdd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.all(v_values_3 >= v_values_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "uh4akjMSHJBF"
      },
      "outputs": [],
      "source": [
        "def value_iteration(env, max_iters=500, gamma=0.9, bool_print = True):\n",
        "    # initialize\n",
        "    v_values = np.zeros(env.observation_space.n)\n",
        "\n",
        "    for i in range(max_iters):\n",
        "        prev_v_values = np.copy(v_values)\n",
        "\n",
        "        # update the v-value for each state\n",
        "        for state in range(env.observation_space.n):\n",
        "            q_values = []\n",
        "\n",
        "            # compute the q-value for each action that we can perform at the state\n",
        "            for action in range(env.action_space.n):\n",
        "                q_value = 0\n",
        "                # loop through each possible outcome\n",
        "                for prob, next_state, reward, done in env.unwrapped.P[state][action]:\n",
        "                    q_value += prob * (reward + gamma * prev_v_values[next_state])\n",
        "\n",
        "                q_values.append(q_value)\n",
        "\n",
        "            # select the max q-values\n",
        "            best_action = np.argmax(q_values)\n",
        "            v_values[state] = q_values[best_action]\n",
        "\n",
        "        # check convergence\n",
        "        if np.all(np.isclose(v_values, prev_v_values)):\n",
        "            if bool_print:\n",
        "                print(f'Converged at {i}-th iteration.')\n",
        "            break\n",
        "\n",
        "    return v_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8xAljw7VuMP",
        "outputId": "e3dd8ca1-e4ab-48ae-c5ef-05e0875f18f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converged at 79-th iteration.\n"
          ]
        }
      ],
      "source": [
        "optimal_v_values = value_iteration(env, max_iters=500, gamma=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7g9VA3lV2WW",
        "outputId": "77522ac0-3a35-422e-9650-dedd92b5e763"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.06888615, 0.06141054, 0.07440682, 0.05580409, 0.09185022,\n",
              "       0.        , 0.11220663, 0.        , 0.14543286, 0.2474946 ,\n",
              "       0.29961593, 0.        , 0.        , 0.3799342 , 0.63901926,\n",
              "       0.        ])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "optimal_v_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "jb0an7gaV39e"
      },
      "outputs": [],
      "source": [
        "def policy_extraction(env, v_values, gamma=0.9):\n",
        "    # initialize\n",
        "    policy = np.zeros(env.observation_space.n, dtype=np.int32)\n",
        "\n",
        "    # loop through each state in the environment\n",
        "    for state in range(env.observation_space.n):\n",
        "        q_values = []\n",
        "        # loop through each action\n",
        "        for action in range(env.action_space.n):\n",
        "            q_value = 0\n",
        "            # loop each possible outcome\n",
        "            for prob, next_state, reward, done in env.unwrapped.P[state][action]:\n",
        "                q_value += prob * (reward + gamma * v_values[next_state])\n",
        "\n",
        "            q_values.append(q_value)\n",
        "\n",
        "        # select the best action\n",
        "        best_action = np.argmax(q_values)\n",
        "        policy[state] = best_action\n",
        "\n",
        "    return policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "7TGCF4G7XErH"
      },
      "outputs": [],
      "source": [
        "optimal_policy = policy_extraction(env, optimal_v_values, gamma=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkHYtfm4qikV",
        "outputId": "5ca7c235-818f-4347-9995-89da1f23fb7b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 3, 0, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0])"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "optimal_policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ww12Uh5qCUb",
        "outputId": "57ac0553-f6d8-43ca-a14f-1c238990a33b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  (Left)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.0, 102)"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "play(env, optimal_policy, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-m4ZqWZXKqG",
        "outputId": "f5e40d21-d8c8-4432-98db-1dde9c039944"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(791, 44.007585335018966)"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "play_multiple_times(env, optimal_policy, 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "YOQ7Hs4DqX2T"
      },
      "outputs": [],
      "source": [
        "def policy_iteration(env, max_iters=500, gamma=0.9, bool_print = True):\n",
        "    # Initialize a random policy\n",
        "    policy = np.random.randint(0, env.action_space.n, env.observation_space.n)\n",
        "    print(\"Initialize a random policy:\", policy)\n",
        "\n",
        "    for i in range(max_iters):\n",
        "        # Policy Evaluation\n",
        "        v_values = policy_evaluation(env, policy, max_iters, gamma, False)\n",
        "\n",
        "        # Policy Improvement\n",
        "        new_policy = policy_extraction(env, v_values, gamma)\n",
        "\n",
        "        # Check if the policy has converged\n",
        "        if np.array_equal(policy, new_policy):\n",
        "            if bool_print:\n",
        "                print(f'Policy is converged at {i}-th iteration.')\n",
        "            break\n",
        "        else:\n",
        "            policy = new_policy\n",
        "    \n",
        "    return policy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initialize a random policy: [0 3 2 1 3 3 2 1 3 3 1 0 2 3 1 2]\n",
            "Policy is converged at 3-th iteration.\n"
          ]
        }
      ],
      "source": [
        "optimal_policy_pi = policy_iteration(env, 100, 0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 3, 0, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0])"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "optimal_policy_pi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(1.0, 65)"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "play(env, optimal_policy_pi, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(7862, 43.39646400407021)"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "play_multiple_times(env, optimal_policy_pi, 10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experiment with FrozenLake-v1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_trials_for_FrozenLake = 1000000\n",
        "num_trials_for_Taxi = 2000000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "env_1 = gym.make('FrozenLake-v1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converged at 79-th iteration.\n",
            "Policy:  [0 3 0 3 0 0 0 0 3 1 0 0 0 2 1 0] \n",
            "\n",
            "Number of successes of Value Iteration in map FrozenLake-v1 : 780719/1000000, Time to find policy: 0.020755290985107422, Total time: 295.8431067466736s, Average time : 0.0002958431067466736s, Average steps : 43.57424118024539 steps\n"
          ]
        }
      ],
      "source": [
        "time_find_optimal_policy_start = time.time()\n",
        "vi_value = value_iteration(env_1, 500, 0.9)\n",
        "policy_from_vi_value = policy_extraction(env_1, vi_value, 0.9)\n",
        "time_find_optimal_policy_end = time.time() - time_find_optimal_policy_start\n",
        "print(\"Policy: \", policy_from_vi_value, \"\\n\")\n",
        "start_vi = time.time()\n",
        "vi_number_of_successes, vi_mean_of_steps = play_multiple_times(env_1, policy_from_vi_value, num_trials_for_FrozenLake)\n",
        "vi_time = time.time() - start_vi\n",
        "print(f'Number of successes of Value Iteration in map FrozenLake-v1 : {vi_number_of_successes}/{num_trials_for_FrozenLake}, Time to find policy: {time_find_optimal_policy_end}, Total time: {vi_time}s, Average time : {vi_time/num_trials_for_FrozenLake}s, Average steps : {vi_mean_of_steps} steps')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initialize a random policy: [0 1 2 3 3 3 2 1 3 0 2 3 3 2 2 3]\n",
            "Policy is converged at 3-th iteration.\n",
            "Policy:  [0 3 0 3 0 0 0 0 3 1 0 0 0 2 1 0] \n",
            "\n",
            "Number of successes of Policy Iteration in map FrozenLake-v1 : 780993/1000000, Time to find policy: 0.014512777328491211, Total time: 301.03059577941895s, Average time : 0.00030103059577941895s, Average steps : 43.49639369366947 steps\n"
          ]
        }
      ],
      "source": [
        "time_find_optimal_policy_start = time.time()\n",
        "policy_pi = policy_iteration(env_1, 500, 0.9)\n",
        "time_find_optimal_policy_end = time.time() - time_find_optimal_policy_start\n",
        "print(\"Policy: \", policy_pi, \"\\n\")\n",
        "start_pi = time.time()\n",
        "pi_number_of_successes, pi_mean_of_steps = play_multiple_times(env_1, policy_pi, num_trials_for_FrozenLake)\n",
        "pi_time = time.time() - start_pi\n",
        "print(f'Number of successes of Policy Iteration in map FrozenLake-v1 : {pi_number_of_successes}/{num_trials_for_FrozenLake}, Time to find policy: {time_find_optimal_policy_end}, Total time: {pi_time}s, Average time : {pi_time/num_trials_for_FrozenLake}s, Average steps : {pi_mean_of_steps} steps')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experiment with FrozenLake8x8-v1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "env_2 = gym.make('FrozenLake8x8-v1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converged at 117-th iteration.\n",
            "Policy:  [3 2 2 2 2 2 2 2 3 3 3 3 2 2 2 1 3 3 0 0 2 3 2 1 3 3 3 1 0 0 2 1 3 3 0 0 2\n",
            " 1 3 2 0 0 0 1 3 0 0 2 0 0 1 0 0 0 0 2 0 1 0 0 1 1 1 0] \n",
            "\n",
            "Number of successes of Value Iteration in map FrozenLake8x8-v1 : 748864/1000000, Time to find policy: 0.08572936058044434, Total time: 502.4876980781555s, Average time : 0.0005024876980781555s, Average steps : 74.54548756516537 steps\n"
          ]
        }
      ],
      "source": [
        "time_find_optimal_policy_start = time.time()\n",
        "vi_value = value_iteration(env_2, 500, 0.9)\n",
        "policy_from_vi_value = policy_extraction(env_2, vi_value, 0.9)\n",
        "time_find_optimal_policy_end = time.time() - time_find_optimal_policy_start\n",
        "print(\"Policy: \", policy_from_vi_value, \"\\n\")\n",
        "start_vi = time.time()\n",
        "vi_number_of_successes, vi_mean_of_steps = play_multiple_times(env_2, policy_from_vi_value, num_trials_for_FrozenLake)\n",
        "vi_time = time.time() - start_vi\n",
        "print(f'Number of successes of Value Iteration in map FrozenLake8x8-v1 : {vi_number_of_successes}/{num_trials_for_FrozenLake}, Time to find policy: {time_find_optimal_policy_end}, Total time: {vi_time}s, Average time : {vi_time/num_trials_for_FrozenLake}s, Average steps : {vi_mean_of_steps} steps')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initialize a random policy: [1 1 1 0 0 1 0 1 0 0 3 2 0 0 3 3 0 2 3 0 1 0 2 1 2 1 0 1 2 2 1 3 2 2 3 0 1\n",
            " 1 1 0 1 0 3 0 3 0 3 1 1 3 1 3 3 2 0 3 3 2 1 3 0 2 1 1]\n",
            "Policy is converged at 4-th iteration.\n",
            "Policy:  [3 2 2 2 2 2 2 2 3 3 3 3 2 2 2 1 3 3 0 0 2 3 2 1 3 3 3 1 0 0 2 1 3 3 0 0 2\n",
            " 1 3 2 0 0 0 1 3 0 0 2 0 0 1 0 0 0 0 2 0 1 0 0 1 1 1 0] \n",
            "\n",
            "Number of successes of Policy Iteration in map FrozenLake8x8-v1 : 749386/1000000, Time to find policy: 0.07019329071044922, Total time: 538.7594585418701s, Average time : 0.0005387594585418701s, Average steps : 74.53192480243827 steps\n"
          ]
        }
      ],
      "source": [
        "time_find_optimal_policy_start = time.time()\n",
        "policy_pi = policy_iteration(env_2, 500, 0.9)\n",
        "time_find_optimal_policy_end = time.time() - time_find_optimal_policy_start\n",
        "print(\"Policy: \", policy_pi, \"\\n\")\n",
        "start_pi = time.time()\n",
        "pi_number_of_successes, pi_mean_of_steps = play_multiple_times(env_2, policy_pi, num_trials_for_FrozenLake)\n",
        "pi_time = time.time() - start_pi\n",
        "print(f'Number of successes of Policy Iteration in map FrozenLake8x8-v1 : {pi_number_of_successes}/{num_trials_for_FrozenLake}, Time to find policy: {time_find_optimal_policy_end}, Total time: {pi_time}s, Average time : {pi_time/num_trials_for_FrozenLake}s, Average steps : {pi_mean_of_steps} steps')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experiment with Taxi-v3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "env_3 = gym.make(\"Taxi-v3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converged at 116-th iteration.\n",
            "Policy:  [4 4 4 4 0 0 0 0 0 0 0 0 0 0 0 0 5 0 0 0 3 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 3\n",
            " 0 0 0 0 0 0 0 2 2 2 2 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 2 2 2 0 0 0 0 0 0\n",
            " 0 0 0 2 0 0 0 0 0 0 4 4 4 4 0 0 0 0 0 0 0 0 0 5 0 0 1 1 1 1 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1\n",
            " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1\n",
            " 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 2 2 2 2 0 0 0 0 2 2 2 2 1 2 0 2 1 1\n",
            " 1 1 2 2 2 2 3 3 3 3 2 2 2 2 1 2 3 2 3 3 3 3 1 1 1 1 3 3 3 3 2 2 2 2 3 1 3\n",
            " 2 3 3 3 3 1 1 1 1 3 3 3 3 0 0 0 0 3 1 3 0 3 3 3 3 1 1 1 1 3 3 3 3 0 0 0 0\n",
            " 3 1 3 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1\n",
            " 1 4 4 4 4 1 1 1 1 1 1 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 1 1 1 5 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 1 1 1 3] \n",
            "\n",
            "Number of successes of Value Iteration in map Taxi-v3 : 2000000/2000000, Time to find policy: 0.6374871730804443, Total time: 300.60770988464355s, Average time : 0.00015030385494232179s, Average steps : 13.069453 steps\n"
          ]
        }
      ],
      "source": [
        "time_find_optimal_policy_start = time.time()\n",
        "vi_value = value_iteration(env_3, 500, 0.9)\n",
        "policy_from_vi_value = policy_extraction(env_3, vi_value, 0.9)\n",
        "time_find_optimal_policy_end = time.time() - time_find_optimal_policy_start\n",
        "print(\"Policy: \", policy_from_vi_value, \"\\n\")\n",
        "start_vi = time.time()\n",
        "vi_number_of_successes, vi_mean_of_steps = play_multiple_times(env_3, policy_from_vi_value, num_trials_for_Taxi)\n",
        "vi_time = time.time() - start_vi\n",
        "print(f'Number of successes of Value Iteration in map Taxi-v3 : {vi_number_of_successes}/{num_trials_for_Taxi}, Time to find policy: {time_find_optimal_policy_end}, Total time: {vi_time}s, Average time : {vi_time/num_trials_for_Taxi}s, Average steps : {vi_mean_of_steps} steps')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initialize a random policy: [3 5 1 4 0 3 0 5 2 2 1 2 4 5 2 5 1 2 1 2 0 5 5 0 1 0 4 5 3 4 4 5 5 0 0 3 2\n",
            " 2 0 0 1 5 3 0 4 5 0 0 2 1 2 2 1 0 4 4 5 5 5 4 5 3 2 5 5 0 1 0 1 1 4 1 0 1\n",
            " 4 0 2 0 2 0 1 0 0 5 4 1 4 0 1 2 3 2 2 2 0 5 3 2 1 4 4 4 1 5 2 3 4 4 1 4 3\n",
            " 4 0 3 2 2 0 2 4 3 2 3 3 2 1 5 0 2 4 5 3 2 0 3 1 5 0 1 4 2 5 2 3 4 1 2 1 3\n",
            " 1 5 2 3 1 4 1 3 1 5 5 5 5 5 2 0 5 3 1 1 5 3 4 1 2 4 4 3 3 0 3 5 4 1 0 0 4\n",
            " 0 1 5 1 4 4 2 2 5 5 5 0 2 1 2 2 5 5 5 5 3 3 3 2 4 4 1 1 3 2 0 2 3 2 1 3 4\n",
            " 3 1 2 3 2 4 1 0 2 0 1 5 3 0 4 1 5 1 3 2 3 3 4 1 4 5 1 4 1 1 2 0 2 1 1 2 0\n",
            " 3 5 4 4 4 0 4 1 1 0 2 3 2 2 0 3 0 5 1 2 5 2 1 0 2 4 2 5 2 5 0 2 4 1 4 0 4\n",
            " 1 2 2 1 4 4 5 0 1 1 3 0 5 2 5 4 5 4 5 2 0 5 5 1 5 4 3 3 2 2 4 5 2 4 1 5 5\n",
            " 4 2 5 1 0 1 2 3 5 1 1 3 0 4 4 2 2 5 4 2 4 3 2 5 4 2 4 2 3 4 4 4 3 2 5 3 3\n",
            " 4 1 1 5 1 1 4 0 0 0 3 1 2 1 4 1 1 2 5 2 3 2 5 5 2 5 5 3 1 2 0 2 2 5 4 1 4\n",
            " 2 3 4 2 3 3 0 1 5 1 5 1 3 3 5 2 5 1 0 1 5 1 4 5 1 5 1 2 5 1 0 0 4 5 2 0 4\n",
            " 5 2 0 3 2 4 4 4 5 5 0 0 2 2 3 0 1 3 4 5 4 5 5 4 2 0 5 4 3 2 0 0 4 4 3 0 2\n",
            " 1 3 3 2 5 1 3 2 3 4 5 3 2 1 2 2 5 0 0]\n",
            "Policy is converged at 16-th iteration.\n",
            "Policy:  [4 4 4 4 0 0 0 0 0 0 0 0 0 0 0 0 5 0 0 0 3 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 3\n",
            " 0 0 0 0 0 0 0 2 2 2 2 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 2 2 2 0 0 0 0 0 0\n",
            " 0 0 0 2 0 0 0 0 0 0 4 4 4 4 0 0 0 0 0 0 0 0 0 5 0 0 1 1 1 1 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1\n",
            " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1\n",
            " 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 2 2 2 2 0 0 0 0 2 2 2 2 1 2 0 2 1 1\n",
            " 1 1 2 2 2 2 3 3 3 3 2 2 2 2 1 2 3 2 3 3 3 3 1 1 1 1 3 3 3 3 2 2 2 2 3 1 3\n",
            " 2 3 3 3 3 1 1 1 1 3 3 3 3 0 0 0 0 3 1 3 0 3 3 3 3 1 1 1 1 3 3 3 3 0 0 0 0\n",
            " 3 1 3 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1\n",
            " 1 4 4 4 4 1 1 1 1 1 1 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 1 1 1 5 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 1 1 1 3] \n",
            "\n",
            "Number of successes of Policy Iteration in map Taxi-v3 : 2000000/2000000, Time to find policy: 1.128133773803711, Total time: 302.7641043663025s, Average time : 0.00015138205218315125s, Average steps : 13.0705495 steps\n"
          ]
        }
      ],
      "source": [
        "time_find_optimal_policy_start = time.time()\n",
        "policy_pi = policy_iteration(env_3, 500, 0.9)\n",
        "time_find_optimal_policy_end = time.time() - time_find_optimal_policy_start\n",
        "print(\"Policy: \", policy_pi, \"\\n\")\n",
        "start_pi = time.time()\n",
        "pi_number_of_successes, pi_mean_of_steps = play_multiple_times(env_3, policy_pi, num_trials_for_Taxi)\n",
        "pi_time = time.time() - start_pi\n",
        "print(f'Number of successes of Policy Iteration in map Taxi-v3 : {pi_number_of_successes}/{num_trials_for_Taxi}, Time to find policy: {time_find_optimal_policy_end}, Total time: {pi_time}s, Average time : {pi_time/num_trials_for_Taxi}s, Average steps : {pi_mean_of_steps} steps')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
