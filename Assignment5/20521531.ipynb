{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20521531.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#  Assignment 4 - Student Information\n",
        "\n",
        "Le Phuoc Vinh Linh\n",
        "\n",
        "Student ID: 20521531\n",
        "\n",
        "Class: CS106.M21.KHTN\n"
      ],
      "metadata": {
        "id": "IBeytvdCDeTL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import necessary libraries and tools"
      ],
      "metadata": {
        "id": "v4Yn2SYdE7tC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhSyhfEy4XSD"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import time\n",
        "from IPython import display"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize FrozenLake-v0 environment"
      ],
      "metadata": {
        "id": "XoFG4T1LFKgT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHf1dAVKAcZm"
      },
      "source": [
        "env = gym.make('FrozenLake-v0')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-6usoQHAmqh",
        "outputId": "16003e9f-33f9-4033-c168-d6a6c057deeb"
      },
      "source": [
        "env.P[0][3] # Transition model"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.3333333333333333, 1, 0.0, False),\n",
              " (0.3333333333333333, 0, 0.0, False),\n",
              " (0.3333333333333333, 0, 0.0, False)]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The observed state space is 16"
      ],
      "metadata": {
        "id": "SdoVdwDLGg-u"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wh7Su0h0AqQz",
        "outputId": "c29c2da5-9dc8-4278-bc97-79565110aac5"
      },
      "source": [
        "env.observation_space.n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Action space is 4"
      ],
      "metadata": {
        "id": "27Ch7R3TG77_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZ68w5bpBScC",
        "outputId": "0e3abf65-4794-409b-fead-3ed4ded5678d"
      },
      "source": [
        "env.action_space.n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement"
      ],
      "metadata": {
        "id": "Puti4V0ag_qz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement Value Iteration algorithm"
      ],
      "metadata": {
        "id": "hNqIK_X8HB3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def value_iteration(env, max_iters, gamma):\n",
        "    # initialize\n",
        "    v_values = np.zeros(env.observation_space.n)\n",
        "\n",
        "    for i in range(max_iters):\n",
        "        prev_v_values = np.copy(v_values)\n",
        "\n",
        "        # Calculate value of state and update the v-value for each state\n",
        "        for state in range(env.observation_space.n):\n",
        "            q_values = []\n",
        "\n",
        "            # Calculate q-value for each action that we can perform at the state\n",
        "            for action in range(env.action_space.n):\n",
        "                q_value = 0\n",
        "                # Loop through each possible outcome\n",
        "                for prob, next_state, reward, done in env.P[state][action]:\n",
        "                    q_value += prob * (reward + gamma * prev_v_values[next_state])\n",
        "                \n",
        "                q_values.append(q_value)\n",
        "            \n",
        "            # Get the best action, select the max q-values\n",
        "            best_action = np.argmax(q_values)\n",
        "            v_values[state] = q_values[best_action]\n",
        "        \n",
        "        # Check convergence\n",
        "        if np.all(np.isclose(v_values, prev_v_values)):\n",
        "            print(f'Converged at {i}-th iteration.')\n",
        "            break\n",
        "    \n",
        "    return v_values"
      ],
      "metadata": {
        "id": "t2T3V8OsHBND"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v_values = value_iteration(env, max_iters=1000, gamma=0.9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yqoZRYWHfUg",
        "outputId": "bb9e305c-655e-4f09-973b-8813678d393c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at 79-th iteration.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement Policy Extraction Method"
      ],
      "metadata": {
        "id": "EQQgCP8jIcBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_extraction(env, v_values, gamma=0.9):\n",
        "     # initialize\n",
        "    policy = np.zeros(env.observation_space.n, dtype=np.int)\n",
        "\n",
        "    # loop through each state in the environment\n",
        "    for state in range(env.observation_space.n):\n",
        "        q_values = []\n",
        "        # Calculate q_value for each action\n",
        "        for action in range(env.action_space.n):\n",
        "            q_value = 0\n",
        "            # loop each possible outcome\n",
        "            for prob, next_state, reward, done in env.P[state][action]:\n",
        "                q_value += prob * (reward + gamma * v_values[next_state])\n",
        "            q_values.append(q_value)\n",
        "        \n",
        "        # Select the best action\n",
        "        best_action = np.argmax(q_values)\n",
        "        policy[state] = best_action\n",
        "    \n",
        "    return policy"
      ],
      "metadata": {
        "id": "-EISFPXsIiGB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "policy = policy_extraction(env, v_values, gamma=0.9)\n",
        "policy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8jhN2iOJBxG",
        "outputId": "2b808ea2-bbdf-4375-e7f6-d99947ccfdf4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 3, 0, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement Policy Iteration Algorithm"
      ],
      "metadata": {
        "id": "Km-bHq2VJWum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_iteration(env, max_iters, gamma):\n",
        "\n",
        "    # Initialization\n",
        "    ini_pi = np.array([env.action_space.sample() for i in range(env.observation_space.n)])\n",
        "\n",
        "    for i in range(max_iters):\n",
        "        # Policy Evaluation\n",
        "        v_values = np.zeros(env.observation_space.n)\n",
        "\n",
        "        for j in range(max_iters):\n",
        "            prev_v_values = np.copy(v_values)\n",
        "\n",
        "            # Calculate value of state\n",
        "            for state, action in enumerate(ini_pi):\n",
        "                # Calculate q-value for each action\n",
        "                q_value = 0\n",
        "                # Loop through each possible outcome\n",
        "                for prob, next_state, reward, done in env.P[state][action]:\n",
        "                    q_value += prob * (reward + gamma * prev_v_values[next_state])\n",
        "                v_values[state] = q_value\n",
        "          \n",
        "            # Check for convergence\n",
        "            if np.all(np.isclose(v_values, prev_v_values)):\n",
        "                break\n",
        "\n",
        "        # Policy Improvement\n",
        "        prev_pi = np.copy(ini_pi)\n",
        "        for state in range(env.observation_space.n):\n",
        "            q_values = []\n",
        "            # Calculate q-value for each action\n",
        "            for action in range(env.action_space.n):\n",
        "                q_value = 0\n",
        "                # Loop through each possible outcome\n",
        "                for prob, next_state, reward, done in env.P[state][action]:\n",
        "                    q_value += prob * (reward + gamma * v_values[next_state])\n",
        "            \n",
        "                q_values.append(q_value)\n",
        "\n",
        "            # Get the best action\n",
        "            best_action = np.argmax(q_values)\n",
        "            ini_pi[state] = best_action\n",
        "            \n",
        "        # Check convergence\n",
        "        if np.all(np.isclose(ini_pi, prev_pi)):\n",
        "            print(f'Converged at {i}-th iteration.')\n",
        "            break\n",
        "    \n",
        "    return ini_pi"
      ],
      "metadata": {
        "id": "OSgnkAhSJbFx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pi = policy_iteration(env, max_iters=1000, gamma=0.9)\n",
        "pi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRdy_pmgJlI5",
        "outputId": "220a812e-3f95-4a01-e60d-1f9cb4de5f74"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at 5-th iteration.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 3, 0, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Play function"
      ],
      "metadata": {
        "id": "e_w6YLJxNLvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def play(env, policy, render=False):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "    steps = 0\n",
        "    #time.sleep(1)\n",
        "    \n",
        "    while not done:\n",
        "        action = policy[state]\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "        \n",
        "        if render:\n",
        "            env.render()\n",
        "            time.sleep(0.2)\n",
        "            if not done:\n",
        "                display.clear_output(wait=True)\n",
        "        state = next_state\n",
        "\n",
        "    return total_reward"
      ],
      "metadata": {
        "id": "WOmbQ0BDKENA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "play(env, policy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqjYcEj2KGru",
        "outputId": "6ae98364-ac64-4f21-fdee-3edc2bfa738e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Play_multiple_times function"
      ],
      "metadata": {
        "id": "l-1piKLzNQdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def play_multiple_times(env, policy, max_episodes):\n",
        "    success = 0\n",
        "\n",
        "    for i in range(max_episodes):\n",
        "        reward = play(env, policy)\n",
        "\n",
        "        if reward > 0:\n",
        "            success += 1\n",
        "    print(f'Number of successes: {success}/{max_episodes}')\n",
        "    return success"
      ],
      "metadata": {
        "id": "6vluwickN-KE"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "play_multiple_times(env, policy, 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNPCYLK4Mbk7",
        "outputId": "36c67edf-9386-4d26-ab7e-1e4416193987"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 732/1000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "732"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The preceding calculation indicates that running 1000 times will result in 732 successes, but we need a more precise figure."
      ],
      "metadata": {
        "id": "fSq_j4MKM3dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run 2000 episodes, each eponodes run 1000 loops"
      ],
      "metadata": {
        "id": "bPyxz_TJM_rC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initilize parameters\n",
        "MAX_ITERS = 1000\n",
        "MAX_EPISODES = 2000\n",
        "GAMMA = 0.9"
      ],
      "metadata": {
        "id": "RTfpZ88xNAP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment"
      ],
      "metadata": {
        "id": "p77VEEIpPRya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment with FrozenLake-v0 "
      ],
      "metadata": {
        "id": "lZX0lY2SNbpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('FrozenLake-v0')\n",
        "\n",
        "vi_value = value_iteration(env, max_iters=MAX_ITERS, gamma=GAMMA)\n",
        "policy_from_value = policy_extraction(env, vi_value, GAMMA)\n",
        "start_vi = time.time()\n",
        "vi_number_of_successes = play_multiple_times(env, policy=policy_from_value, max_episodes=MAX_EPISODES)\n",
        "vi_time = time.time() - start_vi\n",
        "\n",
        "pi = policy_iteration(env, max_iters=MAX_ITERS, gamma=GAMMA)\n",
        "start_pi = time.time()\n",
        "pi_number_of_successes = play_multiple_times(env, policy=pi, max_episodes=MAX_EPISODES)\n",
        "pi_time = time.time() - start_pi\n",
        "\n",
        "print(f'Number of successes of Value Iteration in FrozenLake8x8-v0 : {vi_number_of_successes}/{MAX_EPISODES}, Average time : {vi_time/MAX_EPISODES}s')\n",
        "print(f'Number of successes of Policy Iteration in FrozenLake8x8-v0 : {pi_number_of_successes}/{MAX_EPISODES}, Average time : {pi_time/MAX_EPISODES}s')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZa3G3BANej-",
        "outputId": "103ea8d5-8818-4d97-bedf-ab7219499253"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at 79-th iteration.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 1476/2000\n",
            "Converged at 2-th iteration.\n",
            "Number of successes: 1436/2000\n",
            "Number of successes of Value Iteration in FrozenLake8x8-v0 : 1476/2000, Average time : 0.0004789602756500244s\n",
            "Number of successes of Policy Iteration in FrozenLake8x8-v0 : 1436/2000, Average time : 0.0004445765018463135s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment with FrozenLake8x8-v0 "
      ],
      "metadata": {
        "id": "SwUgyBBYOT3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('FrozenLake8x8-v0')\n",
        "\n",
        "vi_value = value_iteration(env, max_iters=MAX_ITERS, gamma=GAMMA)\n",
        "policy_from_value = policy_extraction(env, vi_value, GAMMA)\n",
        "start_vi = time.time()\n",
        "vi_number_of_successes = play_multiple_times(env, policy=policy_from_value, max_episodes=MAX_EPISODES)\n",
        "vi_time = time.time() - start_vi\n",
        "\n",
        "pi = policy_iteration(env, max_iters=MAX_ITERS, gamma=GAMMA)\n",
        "start_pi = time.time()\n",
        "pi_number_of_successes = play_multiple_times(env, policy=pi, max_episodes=MAX_EPISODES)\n",
        "pi_time = time.time() - start_pi\n",
        "\n",
        "print(f'Number of successes of Value Iteration in FrozenLake8x8-v0 : {vi_number_of_successes}/{MAX_EPISODES}, Average time :  {vi_time/MAX_EPISODES}s')\n",
        "print(f'Number of successes of Policy Iteration in FrozenLake8x8-v0 : {pi_number_of_successes}/{MAX_EPISODES}, Average time : {pi_time/MAX_EPISODES}s')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ii7IUYjbOXXs",
        "outputId": "89822783-e528-418b-d777-e1ae30a59d8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at 117-th iteration.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 1508/2000\n",
            "Converged at 3-th iteration.\n",
            "Number of successes: 1492/2000\n",
            "Number of successes of Value Iteration in FrozenLake8x8-v0 : 1508/2000, Average time :  0.0006862468719482422s\n",
            "Number of successes of Policy Iteration in FrozenLake8x8-v0 : 1492/2000, Average time : 0.0008852777481079101s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment with Taxi-v3"
      ],
      "metadata": {
        "id": "4xmqAHGyPHBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('Taxi-v3')\n",
        "\n",
        "vi_value = value_iteration(env, max_iters=MAX_ITERS, gamma=GAMMA)\n",
        "policy_from_value = policy_extraction(env, vi_value, GAMMA)\n",
        "start_vi = time.time()\n",
        "vi_number_of_successes = play_multiple_times(env, policy=policy_from_value, max_episodes=MAX_EPISODES)\n",
        "vi_time = time.time() - start_vi\n",
        "\n",
        "pi = policy_iteration(env, max_iters=MAX_ITERS, gamma=GAMMA)\n",
        "start_pi = time.time()\n",
        "pi_number_of_successes = play_multiple_times(env, policy=pi, max_episodes=MAX_EPISODES)\n",
        "pi_time = time.time() - start_pi\n",
        "\n",
        "print(f'Number of successes of Value Iteration in FrozenLake8x8-v0 : {vi_number_of_successes}/{MAX_EPISODES}, Average time : {vi_time/MAX_EPISODES}s')\n",
        "print(f'Number of successes of Policy Iteration in FrozenLake8x8-v0 : {pi_number_of_successes}/{MAX_EPISODES}, Average time : {pi_time/MAX_EPISODES}s')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJAxgkpOPJW9",
        "outputId": "b3d591f6-6bfa-486c-c5c8-9ab72cdbd695"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at 116-th iteration.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 2000/2000\n",
            "Converged at 17-th iteration.\n",
            "Number of successes: 2000/2000\n",
            "Number of successes of Value Iteration in FrozenLake8x8-v0 : 2000/2000, Average time : 0.00013901913166046141s\n",
            "Number of successes of Policy Iteration in FrozenLake8x8-v0 : 2000/2000, Average time : 0.00016633963584899902s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion"
      ],
      "metadata": {
        "id": "G8wTl6EcPiV0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the experimental results, we can see that the results of the number of games won and the score of the 3 maps above with the two types of Value Iteration and Policy Iteration algorithms are quite different. The Policy Iteration algorithm converges faster than the Value Iteration algorithm and the running time is also faster, but the difference is not very large for 2000 EPISODES.\n",
        "\n",
        "From an implementation perspective, Policy Iteration generally looks more \n",
        "complicated but runs faster than Value Iteration. Both of these algorithms guarantee that they will converge to an optimal strategy, but these two algorithms have some distinct characteristics in terms of algorithm implementation, computational cost, execution speed, ...\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RDvt0Vr-QF8-"
      }
    }
  ]
}