{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YczE55EXiYx9",
        "outputId": "420f17aa-05f4-4326-f2ad-20b4fd3c40c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gymnasium in c:\\users\\legion 5 pro\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\legion 5 pro\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\legion 5 pro\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium) (3.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\legion 5 pro\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium) (4.11.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\legion 5 pro\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yhSyhfEy4XSD"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import time\n",
        "from IPython import display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kHf1dAVKAcZm"
      },
      "outputs": [],
      "source": [
        "env = gym.make('FrozenLake-v1', render_mode=\"ansi\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-6usoQHAmqh",
        "outputId": "c37e6f40-9d01-4f50-cce9-99de3a7315fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(0.3333333333333333, 1, 0.0, False),\n",
              " (0.3333333333333333, 0, 0.0, False),\n",
              " (0.3333333333333333, 0, 0.0, False)]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env.unwrapped.P[0][3] # Transition model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wh7Su0h0AqQz",
        "outputId": "2a5b804e-5b7a-4a84-8ff9-09ee97790ca0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env.observation_space.n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZ68w5bpBScC",
        "outputId": "05c0a7b0-d868-46ec-e3f3-8ecf85eb1d24"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env.action_space.n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VWLnvY7VBvIZ"
      },
      "outputs": [],
      "source": [
        "def play(env, policy, render=False):\n",
        "    state, _ = env.reset()\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = policy[state]\n",
        "        next_state, reward, done, info, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "        if render:\n",
        "            print(env.render())\n",
        "            time.sleep(0.5)\n",
        "            if not done:\n",
        "                display.clear_output(wait=True)\n",
        "        state = next_state\n",
        "\n",
        "    return (total_reward, steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcuDDx6rC5YE",
        "outputId": "9707ead1-097f-4517-b9ce-8cddda663a7f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.0, 3)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "policy_0 = np.asarray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
        "play(env, policy_0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJ1CJNPhDGPA",
        "outputId": "abfd77fb-c766-454c-e0d5-a774fee2f2d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.0, 18)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "policy_0 = np.asarray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
        "play(env, policy_0, True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdyjjtGZC9NX",
        "outputId": "a7ae95f8-3060-4c4d-93d9-5fcfd4ce1b55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  (Down)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n",
            "0.0 3\n"
          ]
        }
      ],
      "source": [
        "reward, step = 0, None\n",
        "policy_1 = np.asarray([0, 1, 1, 3, 1, 0, 2, 0, 1, 1, 2, 2, 3, 3, 1, 0])\n",
        "reward, step = play(env, policy_1, True)\n",
        "print(reward, step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tt0VhyMuDasc",
        "outputId": "9c362b93-1aaf-4a84-eb16-d63f58f6265b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.0, 6)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "policy_2 = np.array([1, 1, 1, 3, 0, 1, 2, 3, 1, 1, 2, 3, 2, 2, 1, 3])\n",
        "play(env, policy_2, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hp6qhRFJDxWR",
        "outputId": "66da48db-c2b7-448f-b33c-5055f5f3bd30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(1.0, 35)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "policy_3 = np.array([0, 3, 0, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0])\n",
        "play(env, policy_3, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "JU8Q1qMxD6Po"
      },
      "outputs": [],
      "source": [
        "def play_multiple_times(env, policy, max_episodes):\n",
        "    success = 0\n",
        "    list_of_steps = []\n",
        "    for i in range(max_episodes):\n",
        "        total_reward, steps = play(env, policy)\n",
        "\n",
        "        if total_reward > 0:\n",
        "            success += 1\n",
        "            list_of_steps.append(steps)\n",
        "\n",
        "    # print(f'Number of successes: {success}/{max_episodes}')\n",
        "    # print(f'Average number of steps: {np.mean(list_of_steps)}')\n",
        "    return (success, np.mean(list_of_steps))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G427z17PEmjQ",
        "outputId": "f20e5e7b-19f8-4a98-b0a2-8e85220594a0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Legion 5 Pro\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "c:\\Users\\Legion 5 Pro\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0, nan)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "policy_0 = np.asarray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
        "play_multiple_times(env, policy_0, 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1bkhaFdDmj_",
        "outputId": "02e8420d-466e-4035-9e8f-f61bdf391779"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(59, 10.898305084745763)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "policy_1 = np.asarray([0, 1, 1, 3, 1, 0, 2, 0, 1, 1, 2, 2, 3, 3, 1, 0])\n",
        "play_multiple_times(env, policy_1, 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZYhsb_VEtuR",
        "outputId": "933da714-6efe-453c-f9a5-570feaba301a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(100, 16.22)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "policy_2 = np.array([1, 1, 1, 3, 0, 1, 2, 3, 1, 1, 2, 3, 2, 2, 1, 3])\n",
        "play_multiple_times(env, policy_2, 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvvHdMesEzTH",
        "outputId": "3c21ebe0-7c87-4b0b-ed65-350bf2cd5e1c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(780, 44.30128205128205)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "policy_3 = np.array([0, 3, 0, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0])\n",
        "play_multiple_times(env, policy_3, 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "epsilon = 1e-9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "bSomNpxJE5lP"
      },
      "outputs": [],
      "source": [
        "def policy_evaluation(env, policy, max_iters=500, gamma=0.9, bool_print = True):\n",
        "    # Initialize the values of all states to be 0\n",
        "    v_values = np.zeros(env.observation_space.n)\n",
        "\n",
        "    for i in range(max_iters):\n",
        "        prev_v_values = np.copy(v_values)\n",
        "\n",
        "        # Update the value of each state\n",
        "        for state in range(env.observation_space.n):\n",
        "            action = policy[state]\n",
        "\n",
        "            # Compute the q-value of the action\n",
        "            q_value = 0\n",
        "            for prob, next_state, reward, done in env.unwrapped.P[state][action]:\n",
        "                q_value += prob * (reward + gamma * prev_v_values[next_state])\n",
        "\n",
        "            v_values[state] = q_value # update v-value\n",
        "\n",
        "        # Check convergence\n",
        "        if np.all(np.isclose(v_values, prev_v_values)):\n",
        "            if bool_print:\n",
        "                print(f'Converged at {i}-th iteration.')\n",
        "            break\n",
        "\n",
        "    return v_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7IhqEOgGkQX",
        "outputId": "d8f855b6-cfff-45d4-a433-3a76e84e3e55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converged at 0-th iteration.\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "policy_0 = np.asarray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
        "v_values_0 = policy_evaluation(env, policy_0)\n",
        "print(v_values_0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMjJKI3GGrsN",
        "outputId": "a934f69c-a70b-4e06-f1dc-25eafbeb0c47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converged at 48-th iteration.\n",
            "[0.01904157 0.01519815 0.03161906 0.02371389 0.02538879 0.\n",
            " 0.06648515 0.         0.05924054 0.13822794 0.18999823 0.\n",
            " 0.         0.21152109 0.56684236 0.        ]\n"
          ]
        }
      ],
      "source": [
        "policy_1 = np.asarray([0, 1, 1, 3, 1, 0, 2, 0, 1, 1, 2, 2, 3, 3, 1, 0])\n",
        "v_values_1 = policy_evaluation(env, policy_1)\n",
        "print(v_values_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-26M77nEfcV",
        "outputId": "a49842f4-7901-4576-c52d-5fb414de4f1e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.all(v_values_1 >= v_values_0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l49O1N8QG0S2",
        "outputId": "f6e8d433-9d58-4a46-d5e9-ad7746a33acb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converged at 53-th iteration.\n",
            "[0.02889625 0.01951972 0.03616977 0.0271268  0.04790519 0.\n",
            " 0.07391985 0.         0.08288277 0.19339319 0.21022995 0.\n",
            " 0.         0.35153135 0.62684674 0.        ]\n"
          ]
        }
      ],
      "source": [
        "policy_2 = np.array([1, 1, 1, 3, 0, 1, 2, 3, 1, 1, 2, 3, 2, 2, 1, 3])\n",
        "v_values_2 = policy_evaluation(env, policy_2)\n",
        "print(v_values_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22pRvreGE3Yt",
        "outputId": "07915473-ca14-4a0c-e7c7-027c6adf0498"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.all(v_values_2 >= v_values_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTYYFq6BEXDd",
        "outputId": "12fd0fd0-24d6-48dd-a456-ed9323f50d3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converged at 80-th iteration.\n",
            "[0.06888666 0.06141097 0.07440714 0.05580443 0.09185068 0.\n",
            " 0.11220679 0.         0.14543323 0.24749485 0.29961611 0.\n",
            " 0.         0.37993438 0.63901935 0.        ]\n"
          ]
        }
      ],
      "source": [
        "policy_3 = np.array([0, 3, 0, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0])\n",
        "v_values_3 = policy_evaluation(env, policy_3)\n",
        "print(v_values_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcEfU3NYE7xN",
        "outputId": "78187f65-f5d4-4827-fbd3-3ca0feb9ccdd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.all(v_values_3 >= v_values_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "uh4akjMSHJBF"
      },
      "outputs": [],
      "source": [
        "def value_iteration(env, max_iters=500, gamma=0.9, bool_print = True):\n",
        "    # initialize\n",
        "    v_values = np.zeros(env.observation_space.n)\n",
        "\n",
        "    for i in range(max_iters):\n",
        "        prev_v_values = np.copy(v_values)\n",
        "\n",
        "        # update the v-value for each state\n",
        "        for state in range(env.observation_space.n):\n",
        "            q_values = []\n",
        "\n",
        "            # compute the q-value for each action that we can perform at the state\n",
        "            for action in range(env.action_space.n):\n",
        "                q_value = 0\n",
        "                # loop through each possible outcome\n",
        "                for prob, next_state, reward, done in env.unwrapped.P[state][action]:\n",
        "                    q_value += prob * (reward + gamma * prev_v_values[next_state])\n",
        "\n",
        "                q_values.append(q_value)\n",
        "\n",
        "            # select the max q-values\n",
        "            best_action = np.argmax(q_values)\n",
        "            v_values[state] = q_values[best_action]\n",
        "\n",
        "        # check convergence\n",
        "        if np.all(np.isclose(v_values, prev_v_values)):\n",
        "            if bool_print:\n",
        "                print(f'Converged at {i}-th iteration.')\n",
        "            break\n",
        "\n",
        "    return v_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8xAljw7VuMP",
        "outputId": "e3dd8ca1-e4ab-48ae-c5ef-05e0875f18f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converged at 79-th iteration.\n"
          ]
        }
      ],
      "source": [
        "optimal_v_values = value_iteration(env, max_iters=500, gamma=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7g9VA3lV2WW",
        "outputId": "77522ac0-3a35-422e-9650-dedd92b5e763"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.06888615, 0.06141054, 0.07440682, 0.05580409, 0.09185022,\n",
              "       0.        , 0.11220663, 0.        , 0.14543286, 0.2474946 ,\n",
              "       0.29961593, 0.        , 0.        , 0.3799342 , 0.63901926,\n",
              "       0.        ])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "optimal_v_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "jb0an7gaV39e"
      },
      "outputs": [],
      "source": [
        "def policy_extraction(env, v_values, gamma=0.9):\n",
        "    # initialize\n",
        "    policy = np.zeros(env.observation_space.n, dtype=np.int32)\n",
        "\n",
        "    # loop through each state in the environment\n",
        "    for state in range(env.observation_space.n):\n",
        "        q_values = []\n",
        "        # loop through each action\n",
        "        for action in range(env.action_space.n):\n",
        "            q_value = 0\n",
        "            # loop each possible outcome\n",
        "            for prob, next_state, reward, done in env.unwrapped.P[state][action]:\n",
        "                q_value += prob * (reward + gamma * v_values[next_state])\n",
        "\n",
        "            q_values.append(q_value)\n",
        "\n",
        "        # select the best action\n",
        "        best_action = np.argmax(q_values)\n",
        "        policy[state] = best_action\n",
        "\n",
        "    return policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "7TGCF4G7XErH"
      },
      "outputs": [],
      "source": [
        "optimal_policy = policy_extraction(env, optimal_v_values, gamma=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkHYtfm4qikV",
        "outputId": "5ca7c235-818f-4347-9995-89da1f23fb7b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 3, 0, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0])"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "optimal_policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ww12Uh5qCUb",
        "outputId": "57ac0553-f6d8-43ca-a14f-1c238990a33b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(1.0, 17)"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "play(env, optimal_policy, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-m4ZqWZXKqG",
        "outputId": "f5e40d21-d8c8-4432-98db-1dde9c039944"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(774, 42.28294573643411)"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "play_multiple_times(env, optimal_policy, 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "YOQ7Hs4DqX2T"
      },
      "outputs": [],
      "source": [
        "def policy_iteration(env, max_iters=500, gamma=0.9, bool_print = True):\n",
        "    # Initialize a random policy\n",
        "    policy = np.random.randint(0, env.action_space.n, env.observation_space.n)\n",
        "    print(\"Initialize a random policy:\", policy)\n",
        "\n",
        "    for i in range(max_iters):\n",
        "        # Policy Evaluation\n",
        "        v_values = policy_evaluation(env, policy, max_iters, gamma, False)\n",
        "\n",
        "        # Policy Improvement\n",
        "        new_policy = policy_extraction(env, v_values, gamma)\n",
        "\n",
        "        # Check if the policy has converged\n",
        "        if np.array_equal(policy, new_policy):\n",
        "            if bool_print:\n",
        "                print(f'Policy is converged at {i}-th iteration.')\n",
        "            break\n",
        "        else:\n",
        "            policy = new_policy\n",
        "    \n",
        "    return policy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initialize a random policy: [3 2 3 2 2 0 2 0 2 1 1 2 3 1 1 0]\n",
            "Policy is converged at 2-th iteration.\n"
          ]
        }
      ],
      "source": [
        "optimal_policy_pi = policy_iteration(env, 100, 0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 3, 0, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0])"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "optimal_policy_pi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(1.0, 34)"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "play(env, optimal_policy_pi, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(7786, 43.431030053942976)"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "play_multiple_times(env, optimal_policy_pi, 10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experiment with FrozenLake-v1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_trials_for_FrozenLake = 1000000\n",
        "num_trials_for_Taxi = 2000000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "env_1 = gym.make('FrozenLake-v1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial number  1\n",
            "Converged at 79-th iteration.\n",
            "Policy from Value Iteration:  [0 3 0 3 0 0 0 0 3 1 0 0 0 2 1 0] \n",
            "\n",
            "Initialize a random policy: [2 2 1 3 1 1 0 3 2 2 2 2 2 1 2 3]\n",
            "Policy is converged at 2-th iteration.\n",
            "Policy from Policy Iteration:  [0 3 0 3 0 0 0 0 3 1 0 0 0 2 1 0] \n",
            "\n",
            "Number of successes of Value Iteration in map FrozenLake-v1 : 780459/1000000, Time to find policy: 0.017671585083007812, Total time: 301.55681800842285s, Average time per trial: 0.00030155681800842285s, Average steps: 43.597342076906024 steps\n",
            "Number of successes of Policy Iteration in map FrozenLake-v1 : 780895/1000000, Time to find policy: 0.012042760848999023, Total time: 299.39365696907043s, Average time per trial: 0.0002993936569690704s, Average steps: 43.6270305226695 steps\n",
            "\n",
            "\n",
            "Trial number  2\n",
            "Converged at 79-th iteration.\n",
            "Policy from Value Iteration:  [0 3 0 3 0 0 0 0 3 1 0 0 0 2 1 0] \n",
            "\n",
            "Initialize a random policy: [0 2 0 0 3 1 2 3 0 3 2 3 2 0 0 0]\n",
            "Policy is converged at 5-th iteration.\n",
            "Policy from Policy Iteration:  [0 3 0 3 0 0 0 0 3 1 0 0 0 2 1 0] \n",
            "\n",
            "Number of successes of Value Iteration in map FrozenLake-v1 : 779910/1000000, Time to find policy: 0.01302957534790039, Total time: 299.8990321159363s, Average time per trial: 0.0002998990321159363s, Average steps: 43.615700529548285 steps\n",
            "Number of successes of Policy Iteration in map FrozenLake-v1 : 780458/1000000, Time to find policy: 0.015189409255981445, Total time: 297.6933696269989s, Average time per trial: 0.0002976933696269989s, Average steps: 43.54415612371197 steps\n",
            "\n",
            "\n",
            "Trial number  3\n",
            "Converged at 79-th iteration.\n",
            "Policy from Value Iteration:  [0 3 0 3 0 0 0 0 3 1 0 0 0 2 1 0] \n",
            "\n",
            "Initialize a random policy: [0 1 1 0 1 3 3 2 2 0 3 2 0 0 2 3]\n",
            "Policy is converged at 4-th iteration.\n",
            "Policy from Policy Iteration:  [0 3 0 3 0 0 0 0 3 1 0 0 0 2 1 0] \n",
            "\n",
            "Number of successes of Value Iteration in map FrozenLake-v1 : 780212/1000000, Time to find policy: 0.013575077056884766, Total time: 300.0882704257965s, Average time per trial: 0.0003000882704257965s, Average steps: 43.608329530948 steps\n",
            "Number of successes of Policy Iteration in map FrozenLake-v1 : 780155/1000000, Time to find policy: 0.012592554092407227, Total time: 298.58286476135254s, Average time per trial: 0.0002985828647613525s, Average steps: 43.59593414129244 steps\n",
            "\n",
            "\n",
            "Trial number  4\n",
            "Converged at 79-th iteration.\n",
            "Policy from Value Iteration:  [0 3 0 3 0 0 0 0 3 1 0 0 0 2 1 0] \n",
            "\n",
            "Initialize a random policy: [1 1 3 1 2 1 3 3 1 0 3 3 3 3 3 1]\n",
            "Policy is converged at 2-th iteration.\n",
            "Policy from Policy Iteration:  [0 3 0 3 0 0 0 0 3 1 0 0 0 2 1 0] \n",
            "\n",
            "Number of successes of Value Iteration in map FrozenLake-v1 : 780471/1000000, Time to find policy: 0.012687206268310547, Total time: 297.19474744796753s, Average time per trial: 0.00029719474744796753s, Average steps: 43.60334336573684 steps\n",
            "Number of successes of Policy Iteration in map FrozenLake-v1 : 780107/1000000, Time to find policy: 0.010033130645751953, Total time: 296.4926846027374s, Average time per trial: 0.00029649268460273743s, Average steps: 43.51694447043803 steps\n",
            "\n",
            "\n",
            "Trial number  5\n",
            "Converged at 79-th iteration.\n",
            "Policy from Value Iteration:  [0 3 0 3 0 0 0 0 3 1 0 0 0 2 1 0] \n",
            "\n",
            "Initialize a random policy: [1 2 1 0 2 2 0 3 3 2 1 2 3 1 3 0]\n",
            "Policy is converged at 1-th iteration.\n",
            "Policy from Policy Iteration:  [0 3 0 3 0 0 0 0 3 1 0 0 0 2 1 0] \n",
            "\n",
            "Number of successes of Value Iteration in map FrozenLake-v1 : 780470/1000000, Time to find policy: 0.013119220733642578, Total time: 299.32538533210754s, Average time per trial: 0.00029932538533210756s, Average steps: 43.57660127871667 steps\n",
            "Number of successes of Policy Iteration in map FrozenLake-v1 : 780819/1000000, Time to find policy: 0.005513429641723633, Total time: 301.14346575737s, Average time per trial: 0.00030114346575737s, Average steps: 43.58597062827621 steps\n",
            "\n",
            "\n",
            "\n",
            "Mean number of successes for Value Iteration: 780304.4/1000000\n",
            "Mean time to find policy for Value Iteration: 0.014016532897949218s\n",
            "Mean total time for Value Iteration: 299.61285066604614s\n",
            "Mean average time per trial for Value Iteration: 0.0002996128506660461s\n",
            "Mean average steps for Value Iteration: 43.600263356371165 steps\n",
            "\n",
            "Mean number of successes for Policy Iteration: 780486.8/1000000\n",
            "Mean time to find policy for Policy Iteration: 0.011074256896972657s\n",
            "Mean total time for Policy Iteration: 298.66120834350585s\n",
            "Mean average time per trial for Policy Iteration: 0.0002986612083435059s\n",
            "Mean average steps for Policy Iteration: 43.574007177277636 steps\n"
          ]
        }
      ],
      "source": [
        "successes_vi = []\n",
        "times_to_find_policy_vi = []\n",
        "total_times_vi = []\n",
        "average_times_per_trial_vi = []\n",
        "average_steps_vi = []\n",
        "\n",
        "successes_pi = []\n",
        "times_to_find_policy_pi = []\n",
        "total_times_pi = []\n",
        "average_times_per_trial_pi = []\n",
        "average_steps_pi = []\n",
        "\n",
        "for i in range(5):\n",
        "    print(\"Trial number \", i + 1)\n",
        "    time_find_optimal_policy_vi_start = time.time()\n",
        "    vi_value = value_iteration(env_1, 500, 0.9)\n",
        "    policy_from_vi_value = policy_extraction(env_1, vi_value, 0.9)\n",
        "    time_find_optimal_policy_vi_end = time.time() - time_find_optimal_policy_vi_start\n",
        "    print(\"Policy from Value Iteration: \", policy_from_vi_value, \"\\n\")\n",
        "    start_vi = time.time()\n",
        "    vi_number_of_successes, vi_mean_of_steps = play_multiple_times(env_1, policy_from_vi_value, num_trials_for_FrozenLake)\n",
        "    vi_time = time.time() - start_vi\n",
        "    time_find_optimal_policy_pi_start = time.time()\n",
        "    policy_pi = policy_iteration(env_1, 500, 0.9)\n",
        "    time_find_optimal_policy_pi_end = time.time() - time_find_optimal_policy_pi_start\n",
        "    print(\"Policy from Policy Iteration: \", policy_pi, \"\\n\")\n",
        "    start_pi = time.time()\n",
        "    pi_number_of_successes, pi_mean_of_steps = play_multiple_times(env_1, policy_pi, num_trials_for_FrozenLake)\n",
        "    pi_time = time.time() - start_pi\n",
        "    print(f'Number of successes of Value Iteration in map FrozenLake-v1 : {vi_number_of_successes}/{num_trials_for_FrozenLake}, Time to find policy: {time_find_optimal_policy_vi_end}, Total time: {vi_time}s, Average time per trial: {vi_time/num_trials_for_FrozenLake}s, Average steps: {vi_mean_of_steps} steps')\n",
        "    print(f'Number of successes of Policy Iteration in map FrozenLake-v1 : {pi_number_of_successes}/{num_trials_for_FrozenLake}, Time to find policy: {time_find_optimal_policy_pi_end}, Total time: {pi_time}s, Average time per trial: {pi_time/num_trials_for_FrozenLake}s, Average steps: {pi_mean_of_steps} steps')\n",
        "    print(\"\\n\")\n",
        "    successes_vi.append(vi_number_of_successes)\n",
        "    times_to_find_policy_vi.append(time_find_optimal_policy_vi_end)\n",
        "    total_times_vi.append(vi_time)\n",
        "    average_times_per_trial_vi.append(vi_time / num_trials_for_FrozenLake)\n",
        "    average_steps_vi.append(vi_mean_of_steps)\n",
        "    successes_pi.append(pi_number_of_successes)\n",
        "    times_to_find_policy_pi.append(time_find_optimal_policy_pi_end)\n",
        "    total_times_pi.append(pi_time)\n",
        "    average_times_per_trial_pi.append(pi_time / num_trials_for_FrozenLake)\n",
        "    average_steps_pi.append(pi_mean_of_steps)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "mean_successes_vi = np.mean(successes_vi)\n",
        "mean_time_to_find_policy_vi = np.mean(times_to_find_policy_vi)\n",
        "mean_total_time_vi = np.mean(total_times_vi)\n",
        "mean_average_time_per_trial_vi = np.mean(average_times_per_trial_vi)\n",
        "mean_average_steps_vi = np.mean(average_steps_vi)\n",
        "\n",
        "mean_successes_pi = np.mean(successes_pi)\n",
        "mean_time_to_find_policy_pi = np.mean(times_to_find_policy_pi)\n",
        "mean_total_time_pi = np.mean(total_times_pi)\n",
        "mean_average_time_per_trial_pi = np.mean(average_times_per_trial_pi)\n",
        "mean_average_steps_pi = np.mean(average_steps_pi)\n",
        "\n",
        "print(f'\\nMean number of successes for Value Iteration: {mean_successes_vi}/{num_trials_for_FrozenLake}')\n",
        "print(f'Mean time to find policy for Value Iteration: {mean_time_to_find_policy_vi}s')\n",
        "print(f'Mean total time for Value Iteration: {mean_total_time_vi}s')\n",
        "print(f'Mean average time per trial for Value Iteration: {mean_average_time_per_trial_vi}s')\n",
        "print(f'Mean average steps for Value Iteration: {mean_average_steps_vi} steps')\n",
        "\n",
        "print(f'\\nMean number of successes for Policy Iteration: {mean_successes_pi}/{num_trials_for_FrozenLake}')\n",
        "print(f'Mean time to find policy for Policy Iteration: {mean_time_to_find_policy_pi}s')\n",
        "print(f'Mean total time for Policy Iteration: {mean_total_time_pi}s')\n",
        "print(f'Mean average time per trial for Policy Iteration: {mean_average_time_per_trial_pi}s')\n",
        "print(f'Mean average steps for Policy Iteration: {mean_average_steps_pi} steps')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experiment with FrozenLake8x8-v1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "env_2 = gym.make('FrozenLake8x8-v1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial number  1\n",
            "Converged at 117-th iteration.\n",
            "Policy from Value Iteration:  [3 2 2 2 2 2 2 2 3 3 3 3 2 2 2 1 3 3 0 0 2 3 2 1 3 3 3 1 0 0 2 1 3 3 0 0 2\n",
            " 1 3 2 0 0 0 1 3 0 0 2 0 0 1 0 0 0 0 2 0 1 0 0 1 1 1 0] \n",
            "\n",
            "Initialize a random policy: [3 0 1 3 0 2 0 1 2 1 0 3 1 2 2 0 0 2 1 2 0 2 1 2 3 1 0 0 3 0 1 0 0 3 1 1 3\n",
            " 0 1 2 1 0 2 0 0 0 3 0 1 3 3 0 1 0 2 3 3 0 1 2 3 1 0 1]\n",
            "Policy is converged at 9-th iteration.\n",
            "Policy from Policy Iteration:  [3 2 2 2 2 2 2 2 3 3 3 3 2 2 2 1 3 3 0 0 2 3 2 1 3 3 3 1 0 0 2 1 3 3 0 0 2\n",
            " 1 3 2 0 0 0 1 3 0 0 2 0 0 1 0 0 0 0 2 0 1 0 0 1 1 1 0] \n",
            "\n",
            "Number of successes of Policy Iteration in map FrozenLake8x8-v1 : 748544/1000000, Time to find policy: 0.09699511528015137, Total time: 496.00624918937683s, Average time per trial: 0.0004960062491893768s, Average steps: 74.52327986063611 steps\n",
            "Number of successes of Value Iteration in map FrozenLake8x8-v1 : 748977/1000000, Time to find policy: 0.09020614624023438, Total time: 493.1849482059479s, Average time per trial: 0.0004931849482059479s, Average steps: 74.61012287426716 steps\n",
            "\n",
            "\n",
            "Trial number  2\n",
            "Converged at 117-th iteration.\n",
            "Policy from Value Iteration:  [3 2 2 2 2 2 2 2 3 3 3 3 2 2 2 1 3 3 0 0 2 3 2 1 3 3 3 1 0 0 2 1 3 3 0 0 2\n",
            " 1 3 2 0 0 0 1 3 0 0 2 0 0 1 0 0 0 0 2 0 1 0 0 1 1 1 0] \n",
            "\n",
            "Initialize a random policy: [3 2 1 2 2 2 0 1 2 1 3 1 3 3 2 1 0 1 0 0 0 1 1 2 3 3 1 0 1 0 2 2 0 1 3 3 2\n",
            " 0 3 0 3 2 0 2 1 2 3 1 2 0 2 2 1 2 2 0 3 1 1 3 0 0 2 0]\n",
            "Policy is converged at 3-th iteration.\n",
            "Policy from Policy Iteration:  [3 2 2 2 2 2 2 2 3 3 3 3 2 2 2 1 3 3 0 0 2 3 2 1 3 3 3 1 0 0 2 1 3 3 0 0 2\n",
            " 1 3 2 0 0 0 1 3 0 0 2 0 0 1 0 0 0 0 2 0 1 0 0 1 1 1 0] \n",
            "\n",
            "Number of successes of Policy Iteration in map FrozenLake8x8-v1 : 748145/1000000, Time to find policy: 0.05399918556213379, Total time: 493.34175634384155s, Average time per trial: 0.0004933417563438415s, Average steps: 74.5615970166211 steps\n",
            "Number of successes of Value Iteration in map FrozenLake8x8-v1 : 748963/1000000, Time to find policy: 0.06640934944152832, Total time: 494.6828076839447s, Average time per trial: 0.0004946828076839447s, Average steps: 74.5588219978824 steps\n",
            "\n",
            "\n",
            "Trial number  3\n",
            "Converged at 117-th iteration.\n",
            "Policy from Value Iteration:  [3 2 2 2 2 2 2 2 3 3 3 3 2 2 2 1 3 3 0 0 2 3 2 1 3 3 3 1 0 0 2 1 3 3 0 0 2\n",
            " 1 3 2 0 0 0 1 3 0 0 2 0 0 1 0 0 0 0 2 0 1 0 0 1 1 1 0] \n",
            "\n",
            "Initialize a random policy: [0 3 0 0 1 0 0 1 1 0 3 1 2 2 3 1 1 1 2 1 1 3 1 1 2 2 2 2 2 1 1 3 2 0 1 2 0\n",
            " 1 2 0 3 2 2 3 1 1 2 3 2 2 1 2 1 1 1 1 1 2 3 1 0 2 2 0]\n",
            "Policy is converged at 4-th iteration.\n",
            "Policy from Policy Iteration:  [3 2 2 2 2 2 2 2 3 3 3 3 2 2 2 1 3 3 0 0 2 3 2 1 3 3 3 1 0 0 2 1 3 3 0 0 2\n",
            " 1 3 2 0 0 0 1 3 0 0 2 0 0 1 0 0 0 0 2 0 1 0 0 1 1 1 0] \n",
            "\n",
            "Number of successes of Policy Iteration in map FrozenLake8x8-v1 : 748063/1000000, Time to find policy: 0.05458235740661621, Total time: 491.84184885025024s, Average time per trial: 0.0004918418488502502s, Average steps: 74.49794602860989 steps\n",
            "Number of successes of Value Iteration in map FrozenLake8x8-v1 : 748815/1000000, Time to find policy: 0.06531143188476562, Total time: 492.9448161125183s, Average time per trial: 0.0004929448161125183s, Average steps: 74.51690738032758 steps\n",
            "\n",
            "\n",
            "Trial number  4\n",
            "Converged at 117-th iteration.\n",
            "Policy from Value Iteration:  [3 2 2 2 2 2 2 2 3 3 3 3 2 2 2 1 3 3 0 0 2 3 2 1 3 3 3 1 0 0 2 1 3 3 0 0 2\n",
            " 1 3 2 0 0 0 1 3 0 0 2 0 0 1 0 0 0 0 2 0 1 0 0 1 1 1 0] \n",
            "\n",
            "Initialize a random policy: [0 0 2 3 3 0 3 2 2 2 3 1 3 2 0 2 3 3 0 3 0 2 0 0 1 2 1 1 0 1 3 2 1 3 1 3 0\n",
            " 3 2 1 2 2 0 2 0 0 1 2 3 1 0 3 0 3 2 3 0 1 3 1 3 2 0 1]\n",
            "Policy is converged at 9-th iteration.\n",
            "Policy from Policy Iteration:  [3 2 2 2 2 2 2 2 3 3 3 3 2 2 2 1 3 3 0 0 2 3 2 1 3 3 3 1 0 0 2 1 3 3 0 0 2\n",
            " 1 3 2 0 0 0 1 3 0 0 2 0 0 1 0 0 0 0 2 0 1 0 0 1 1 1 0] \n",
            "\n",
            "Number of successes of Policy Iteration in map FrozenLake8x8-v1 : 748368/1000000, Time to find policy: 0.10083532333374023, Total time: 489.5036542415619s, Average time per trial: 0.0004895036542415619s, Average steps: 74.47193092168558 steps\n",
            "Number of successes of Value Iteration in map FrozenLake8x8-v1 : 749015/1000000, Time to find policy: 0.06768584251403809, Total time: 492.68201756477356s, Average time per trial: 0.0004926820175647736s, Average steps: 74.52600548720653 steps\n",
            "\n",
            "\n",
            "Trial number  5\n",
            "Converged at 117-th iteration.\n",
            "Policy from Value Iteration:  [3 2 2 2 2 2 2 2 3 3 3 3 2 2 2 1 3 3 0 0 2 3 2 1 3 3 3 1 0 0 2 1 3 3 0 0 2\n",
            " 1 3 2 0 0 0 1 3 0 0 2 0 0 1 0 0 0 0 2 0 1 0 0 1 1 1 0] \n",
            "\n",
            "Initialize a random policy: [3 0 1 2 3 3 2 1 2 3 0 3 1 1 0 1 2 2 3 3 0 2 2 1 2 1 2 2 0 0 2 3 0 0 0 2 2\n",
            " 2 0 2 3 2 0 2 1 2 3 1 1 2 1 0 0 2 1 1 1 3 2 3 2 1 2 2]\n",
            "Policy is converged at 3-th iteration.\n",
            "Policy from Policy Iteration:  [3 2 2 2 2 2 2 2 3 3 3 3 2 2 2 1 3 3 0 0 2 3 2 1 3 3 3 1 0 0 2 1 3 3 0 0 2\n",
            " 1 3 2 0 0 0 1 3 0 0 2 0 0 1 0 0 0 0 2 0 1 0 0 1 1 1 0] \n",
            "\n",
            "Number of successes of Policy Iteration in map FrozenLake8x8-v1 : 749548/1000000, Time to find policy: 0.049460649490356445, Total time: 490.91645312309265s, Average time per trial: 0.0004909164531230926s, Average steps: 74.55920768249665 steps\n",
            "Number of successes of Value Iteration in map FrozenLake8x8-v1 : 748429/1000000, Time to find policy: 0.0657949447631836, Total time: 493.1381275653839s, Average time per trial: 0.0004931381275653839s, Average steps: 74.61559747150365 steps\n",
            "\n",
            "\n",
            "\n",
            "Mean number of successes for Value Iteration: 748839.8/1000000\n",
            "Mean time to find policy for Value Iteration: 0.07108154296875s\n",
            "Mean total time for Value Iteration: 493.3265434265137s\n",
            "Mean average time per trial for Value Iteration: 0.0004933265434265136s\n",
            "Mean average steps for Value Iteration: 74.56549104223745 steps\n",
            "\n",
            "Mean number of successes for Policy Iteration: 748533.6/1000000\n",
            "Mean time to find policy for Policy Iteration: 0.0711745262145996s\n",
            "Mean total time for Policy Iteration: 492.3219923496246s\n",
            "Mean average time per trial for Policy Iteration: 0.0004923219923496246s\n",
            "Mean average steps for Policy Iteration: 74.52279230200986 steps\n"
          ]
        }
      ],
      "source": [
        "successes_vi = []\n",
        "times_to_find_policy_vi = []\n",
        "total_times_vi = []\n",
        "average_times_per_trial_vi = []\n",
        "average_steps_vi = []\n",
        "\n",
        "successes_pi = []\n",
        "times_to_find_policy_pi = []\n",
        "total_times_pi = []\n",
        "average_times_per_trial_pi = []\n",
        "average_steps_pi = []\n",
        "\n",
        "for i in range(5):\n",
        "    print(\"Trial number \", i + 1)\n",
        "    time_find_optimal_policy_vi_start = time.time()\n",
        "    vi_value = value_iteration(env_2, 500, 0.9)\n",
        "    policy_from_vi_value = policy_extraction(env_2, vi_value, 0.9)\n",
        "    time_find_optimal_policy_vi_end = time.time() - time_find_optimal_policy_vi_start\n",
        "    print(\"Policy from Value Iteration: \", policy_from_vi_value, \"\\n\")\n",
        "    start_vi = time.time()\n",
        "    vi_number_of_successes, vi_mean_of_steps = play_multiple_times(env_2, policy_from_vi_value, num_trials_for_FrozenLake)\n",
        "    vi_time = time.time() - start_vi\n",
        "    time_find_optimal_policy_pi_start = time.time()\n",
        "    policy_pi = policy_iteration(env_2, 500, 0.9)\n",
        "    time_find_optimal_policy_pi_end = time.time() - time_find_optimal_policy_pi_start\n",
        "    print(\"Policy from Policy Iteration: \", policy_pi, \"\\n\")\n",
        "    start_pi = time.time()\n",
        "    pi_number_of_successes, pi_mean_of_steps = play_multiple_times(env_2, policy_pi, num_trials_for_FrozenLake)\n",
        "    pi_time = time.time() - start_pi\n",
        "    print(f'Number of successes of Policy Iteration in map FrozenLake8x8-v1 : {pi_number_of_successes}/{num_trials_for_FrozenLake}, Time to find policy: {time_find_optimal_policy_pi_end}, Total time: {pi_time}s, Average time per trial: {pi_time/num_trials_for_FrozenLake}s, Average steps: {pi_mean_of_steps} steps')\n",
        "    print(f'Number of successes of Value Iteration in map FrozenLake8x8-v1 : {vi_number_of_successes}/{num_trials_for_FrozenLake}, Time to find policy: {time_find_optimal_policy_vi_end}, Total time: {vi_time}s, Average time per trial: {vi_time/num_trials_for_FrozenLake}s, Average steps: {vi_mean_of_steps} steps')\n",
        "    print(\"\\n\")\n",
        "    successes_vi.append(vi_number_of_successes)\n",
        "    times_to_find_policy_vi.append(time_find_optimal_policy_vi_end)\n",
        "    total_times_vi.append(vi_time)\n",
        "    average_times_per_trial_vi.append(vi_time / num_trials_for_FrozenLake)\n",
        "    average_steps_vi.append(vi_mean_of_steps)\n",
        "    successes_pi.append(pi_number_of_successes)\n",
        "    times_to_find_policy_pi.append(time_find_optimal_policy_pi_end)\n",
        "    total_times_pi.append(pi_time)\n",
        "    average_times_per_trial_pi.append(pi_time / num_trials_for_FrozenLake)\n",
        "    average_steps_pi.append(pi_mean_of_steps)\n",
        "\n",
        "mean_successes_vi = np.mean(successes_vi)\n",
        "mean_time_to_find_policy_vi = np.mean(times_to_find_policy_vi)\n",
        "mean_total_time_vi = np.mean(total_times_vi)\n",
        "mean_average_time_per_trial_vi = np.mean(average_times_per_trial_vi)\n",
        "mean_average_steps_vi = np.mean(average_steps_vi)\n",
        "\n",
        "mean_successes_pi = np.mean(successes_pi)\n",
        "mean_time_to_find_policy_pi = np.mean(times_to_find_policy_pi)\n",
        "mean_total_time_pi = np.mean(total_times_pi)\n",
        "mean_average_time_per_trial_pi = np.mean(average_times_per_trial_pi)\n",
        "mean_average_steps_pi = np.mean(average_steps_pi)\n",
        "\n",
        "print(f'\\nMean number of successes for Value Iteration: {mean_successes_vi}/{num_trials_for_FrozenLake}')\n",
        "print(f'Mean time to find policy for Value Iteration: {mean_time_to_find_policy_vi}s')\n",
        "print(f'Mean total time for Value Iteration: {mean_total_time_vi}s')\n",
        "print(f'Mean average time per trial for Value Iteration: {mean_average_time_per_trial_vi}s')\n",
        "print(f'Mean average steps for Value Iteration: {mean_average_steps_vi} steps')\n",
        "\n",
        "print(f'\\nMean number of successes for Policy Iteration: {mean_successes_pi}/{num_trials_for_FrozenLake}')\n",
        "print(f'Mean time to find policy for Policy Iteration: {mean_time_to_find_policy_pi}s')\n",
        "print(f'Mean total time for Policy Iteration: {mean_total_time_pi}s')\n",
        "print(f'Mean average time per trial for Policy Iteration: {mean_average_time_per_trial_pi}s')\n",
        "print(f'Mean average steps for Policy Iteration: {mean_average_steps_pi} steps')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experiment with Taxi-v3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "env_3 = gym.make(\"Taxi-v3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial number  1\n",
            "Converged at 116-th iteration.\n",
            "Policy from Value Iteration:  [4 4 4 4 0 0 0 0 0 0 0 0 0 0 0 0 5 0 0 0 3 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 3\n",
            " 0 0 0 0 0 0 0 2 2 2 2 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 2 2 2 0 0 0 0 0 0\n",
            " 0 0 0 2 0 0 0 0 0 0 4 4 4 4 0 0 0 0 0 0 0 0 0 5 0 0 1 1 1 1 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1\n",
            " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1\n",
            " 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 2 2 2 2 0 0 0 0 2 2 2 2 1 2 0 2 1 1\n",
            " 1 1 2 2 2 2 3 3 3 3 2 2 2 2 1 2 3 2 3 3 3 3 1 1 1 1 3 3 3 3 2 2 2 2 3 1 3\n",
            " 2 3 3 3 3 1 1 1 1 3 3 3 3 0 0 0 0 3 1 3 0 3 3 3 3 1 1 1 1 3 3 3 3 0 0 0 0\n",
            " 3 1 3 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1\n",
            " 1 4 4 4 4 1 1 1 1 1 1 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 1 1 1 5 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 1 1 1 3] \n",
            "\n",
            "Initialize a random policy: [4 4 2 0 5 2 1 4 3 3 0 5 2 3 2 2 2 5 4 0 4 3 2 3 0 4 2 2 3 4 3 3 0 0 5 5 2\n",
            " 2 1 4 3 2 0 1 3 5 1 3 1 3 5 4 2 3 0 2 3 5 3 2 1 0 4 3 2 4 2 2 2 2 2 3 1 4\n",
            " 3 4 5 2 2 2 0 0 3 3 0 4 2 5 1 0 2 4 2 5 2 3 4 5 4 0 1 2 5 5 2 2 0 3 0 4 1\n",
            " 5 2 5 1 0 4 1 5 3 0 0 1 3 2 1 0 2 5 4 1 3 3 1 4 5 3 0 1 1 0 2 1 1 3 5 1 0\n",
            " 4 0 0 0 4 4 1 5 5 1 1 4 3 5 2 3 4 5 5 4 1 2 1 0 1 2 0 2 1 3 0 3 3 0 1 0 0\n",
            " 4 1 1 0 0 1 5 5 1 2 3 1 2 4 1 5 3 4 3 1 5 5 4 5 5 5 3 4 2 3 5 4 1 5 3 0 0\n",
            " 5 5 3 2 3 3 0 2 5 1 1 5 3 3 3 4 1 3 4 3 1 4 3 5 1 4 4 1 1 3 2 2 4 4 5 0 3\n",
            " 2 5 1 3 5 4 1 0 0 4 0 1 0 0 2 2 1 5 4 4 5 5 0 0 1 4 0 2 0 3 4 1 4 1 3 0 2\n",
            " 0 0 2 2 2 0 4 0 3 1 4 5 5 5 5 0 4 3 5 2 0 4 2 3 1 0 5 0 0 4 1 2 2 0 4 0 2\n",
            " 5 5 0 4 2 0 5 1 0 5 5 0 2 2 3 2 0 0 5 3 4 4 1 0 3 2 0 1 5 3 5 2 0 0 2 3 0\n",
            " 1 0 4 0 2 0 2 0 0 5 0 1 1 0 4 4 0 5 4 1 3 0 3 2 2 0 0 2 2 3 4 3 0 1 3 4 1\n",
            " 0 1 2 2 4 1 2 4 4 0 5 3 5 4 3 5 5 0 1 4 2 2 1 2 5 1 4 3 0 5 2 2 1 2 5 5 5\n",
            " 0 3 1 1 4 4 4 1 5 2 4 0 3 3 2 5 5 1 4 3 0 2 1 0 5 3 5 5 5 2 1 2 3 1 4 1 4\n",
            " 4 1 0 4 4 0 2 1 1 1 0 2 5 3 5 4 5 2 5]\n",
            "Policy is converged at 15-th iteration.\n",
            "Policy from Policy Iteration:  [4 4 4 4 0 0 0 0 0 0 0 0 0 0 0 0 5 0 0 0 3 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 3\n",
            " 0 0 0 0 0 0 0 2 2 2 2 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 2 2 2 0 0 0 0 0 0\n",
            " 0 0 0 2 0 0 0 0 0 0 4 4 4 4 0 0 0 0 0 0 0 0 0 5 0 0 1 1 1 1 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1\n",
            " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1\n",
            " 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 2 2 2 2 0 0 0 0 2 2 2 2 1 2 0 2 1 1\n",
            " 1 1 2 2 2 2 3 3 3 3 2 2 2 2 1 2 3 2 3 3 3 3 1 1 1 1 3 3 3 3 2 2 2 2 3 1 3\n",
            " 2 3 3 3 3 1 1 1 1 3 3 3 3 0 0 0 0 3 1 3 0 3 3 3 3 1 1 1 1 3 3 3 3 0 0 0 0\n",
            " 3 1 3 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1\n",
            " 1 4 4 4 4 1 1 1 1 1 1 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 1 1 1 5 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 1 1 1 3] \n",
            "\n",
            "Number of successes of Policy Iteration in map Taxi-v3 : 2000000/2000000, Time to find policy: 0.8975934982299805, Total time: 255.58544993400574s, Average time per trial: 0.00012779272496700288s, Average steps: 13.0724305 steps\n",
            "Number of successes of Value Iteration in map Taxi-v3 : 2000000/2000000, Time to find policy: 0.5708990097045898, Total time: 250.97544360160828s, Average time per trial: 0.00012548772180080413s, Average steps: 13.0709635 steps\n",
            "\n",
            "\n",
            "Trial number  2\n",
            "Converged at 116-th iteration.\n",
            "Policy from Value Iteration:  [4 4 4 4 0 0 0 0 0 0 0 0 0 0 0 0 5 0 0 0 3 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 3\n",
            " 0 0 0 0 0 0 0 2 2 2 2 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 2 2 2 0 0 0 0 0 0\n",
            " 0 0 0 2 0 0 0 0 0 0 4 4 4 4 0 0 0 0 0 0 0 0 0 5 0 0 1 1 1 1 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1\n",
            " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1\n",
            " 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 2 2 2 2 0 0 0 0 2 2 2 2 1 2 0 2 1 1\n",
            " 1 1 2 2 2 2 3 3 3 3 2 2 2 2 1 2 3 2 3 3 3 3 1 1 1 1 3 3 3 3 2 2 2 2 3 1 3\n",
            " 2 3 3 3 3 1 1 1 1 3 3 3 3 0 0 0 0 3 1 3 0 3 3 3 3 1 1 1 1 3 3 3 3 0 0 0 0\n",
            " 3 1 3 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1\n",
            " 1 4 4 4 4 1 1 1 1 1 1 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 1 1 1 5 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 1 1 1 3] \n",
            "\n",
            "Initialize a random policy: [2 4 1 5 4 0 5 2 3 3 1 1 0 1 1 1 2 2 1 0 2 2 0 4 2 4 5 0 4 3 4 4 2 1 1 4 3\n",
            " 0 4 5 1 5 5 3 5 4 1 5 4 4 0 3 5 4 1 1 5 2 2 2 3 3 3 2 3 5 1 2 3 5 1 2 5 2\n",
            " 5 1 4 0 3 2 3 1 1 5 1 0 1 4 4 0 5 3 2 1 5 5 1 1 4 1 5 0 1 5 5 0 3 3 1 4 0\n",
            " 3 5 4 3 0 5 0 5 2 4 5 3 5 5 5 3 0 2 2 4 3 4 0 1 3 4 0 3 4 0 4 0 0 2 5 5 3\n",
            " 4 5 0 0 4 0 1 4 3 1 4 1 2 2 3 3 5 3 5 2 2 1 1 3 1 2 1 4 5 1 5 2 5 0 2 4 3\n",
            " 2 3 0 4 5 3 0 4 2 0 3 4 1 1 5 5 2 5 4 3 5 4 5 2 5 5 4 3 2 4 4 0 4 1 4 1 1\n",
            " 1 2 1 5 5 2 4 4 5 0 3 2 0 3 3 2 2 4 0 0 4 1 1 5 1 5 2 0 2 1 4 3 1 5 1 2 3\n",
            " 4 2 1 1 0 4 4 4 0 3 4 3 2 2 3 5 5 5 4 2 3 2 2 3 4 2 3 2 3 3 3 5 1 1 5 5 0\n",
            " 0 4 4 4 2 0 3 3 5 1 1 2 2 3 0 3 5 3 3 4 2 5 0 5 2 3 2 1 0 1 0 1 1 4 0 1 2\n",
            " 1 5 2 5 5 3 0 5 2 4 1 0 3 3 5 0 2 5 2 0 3 4 1 1 5 2 0 0 0 4 4 1 3 2 5 1 0\n",
            " 4 1 2 4 5 1 3 2 3 3 2 4 1 3 2 5 3 2 5 1 3 2 5 2 4 1 4 1 5 2 4 3 1 0 0 4 4\n",
            " 0 0 4 3 3 3 2 3 1 3 2 1 1 0 4 2 2 4 0 3 5 1 4 1 0 2 1 5 1 5 4 2 5 0 4 1 4\n",
            " 5 4 1 0 0 2 2 3 4 1 2 0 5 3 0 3 3 0 5 2 4 5 0 2 0 2 1 5 2 1 0 0 1 1 5 3 2\n",
            " 3 5 5 3 0 1 0 3 1 5 5 5 5 2 3 4 1 3 5]\n",
            "Policy is converged at 16-th iteration.\n",
            "Policy from Policy Iteration:  [4 4 4 4 0 0 0 0 0 0 0 0 0 0 0 0 5 0 0 0 3 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 3\n",
            " 0 0 0 0 0 0 0 2 2 2 2 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 2 2 2 0 0 0 0 0 0\n",
            " 0 0 0 2 0 0 0 0 0 0 4 4 4 4 0 0 0 0 0 0 0 0 0 5 0 0 1 1 1 1 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1\n",
            " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1\n",
            " 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 2 2 2 2 0 0 0 0 2 2 2 2 1 2 0 2 1 1\n",
            " 1 1 2 2 2 2 3 3 3 3 2 2 2 2 1 2 3 2 3 3 3 3 1 1 1 1 3 3 3 3 2 2 2 2 3 1 3\n",
            " 2 3 3 3 3 1 1 1 1 3 3 3 3 0 0 0 0 3 1 3 0 3 3 3 3 1 1 1 1 3 3 3 3 0 0 0 0\n",
            " 3 1 3 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1\n",
            " 1 4 4 4 4 1 1 1 1 1 1 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 1 1 1 5 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 1 1 1 3] \n",
            "\n",
            "Number of successes of Policy Iteration in map Taxi-v3 : 2000000/2000000, Time to find policy: 0.9486775398254395, Total time: 251.8026044368744s, Average time per trial: 0.0001259013022184372s, Average steps: 13.068733 steps\n",
            "Number of successes of Value Iteration in map Taxi-v3 : 2000000/2000000, Time to find policy: 0.5540361404418945, Total time: 249.55270147323608s, Average time per trial: 0.00012477635073661805s, Average steps: 13.068201 steps\n",
            "\n",
            "\n",
            "Trial number  3\n",
            "Converged at 116-th iteration.\n",
            "Policy from Value Iteration:  [4 4 4 4 0 0 0 0 0 0 0 0 0 0 0 0 5 0 0 0 3 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 3\n",
            " 0 0 0 0 0 0 0 2 2 2 2 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 2 2 2 0 0 0 0 0 0\n",
            " 0 0 0 2 0 0 0 0 0 0 4 4 4 4 0 0 0 0 0 0 0 0 0 5 0 0 1 1 1 1 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1\n",
            " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1\n",
            " 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 2 2 2 2 0 0 0 0 2 2 2 2 1 2 0 2 1 1\n",
            " 1 1 2 2 2 2 3 3 3 3 2 2 2 2 1 2 3 2 3 3 3 3 1 1 1 1 3 3 3 3 2 2 2 2 3 1 3\n",
            " 2 3 3 3 3 1 1 1 1 3 3 3 3 0 0 0 0 3 1 3 0 3 3 3 3 1 1 1 1 3 3 3 3 0 0 0 0\n",
            " 3 1 3 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1\n",
            " 1 4 4 4 4 1 1 1 1 1 1 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 1 1 1 5 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 1 1 1 3] \n",
            "\n",
            "Initialize a random policy: [2 5 0 2 1 1 0 4 5 2 5 0 0 5 0 1 2 0 2 5 0 3 1 2 2 3 4 0 3 2 3 1 4 1 5 3 3\n",
            " 0 0 3 1 2 3 2 3 2 2 5 1 4 5 0 1 2 2 2 5 3 2 4 5 3 4 5 0 5 0 5 2 1 0 3 1 4\n",
            " 1 3 5 3 3 4 2 1 0 2 5 3 1 0 2 3 0 0 4 1 0 4 1 0 3 2 1 2 0 3 3 0 2 4 2 2 1\n",
            " 2 2 2 5 4 1 4 5 0 5 0 5 4 1 4 5 4 2 3 3 0 4 2 0 3 3 2 1 5 0 3 0 0 3 2 4 4\n",
            " 3 0 5 5 4 5 2 5 3 3 0 0 1 4 2 1 4 1 4 0 3 3 2 2 5 2 4 3 2 2 1 1 5 1 4 0 1\n",
            " 1 0 3 4 2 4 1 0 4 3 3 5 3 0 4 3 0 3 1 2 0 3 5 1 2 1 3 0 4 2 5 2 4 3 5 5 5\n",
            " 1 3 4 4 1 1 5 3 0 5 2 4 4 4 3 4 5 4 4 1 5 3 2 0 3 0 0 5 2 5 2 1 2 5 0 5 4\n",
            " 0 1 3 1 0 1 5 5 1 1 5 5 2 1 1 4 0 3 1 5 4 4 1 0 4 4 0 4 1 1 0 2 5 1 4 5 5\n",
            " 5 2 3 1 3 2 2 0 5 1 1 4 2 0 5 0 0 4 0 5 5 1 2 1 4 2 1 1 0 0 3 4 4 0 4 3 5\n",
            " 2 0 0 4 4 0 5 3 0 2 2 3 0 5 0 0 2 1 4 2 3 3 1 2 5 1 3 5 0 0 4 2 5 5 5 2 0\n",
            " 3 1 3 1 0 2 4 0 1 5 4 0 0 2 1 0 0 2 1 2 4 3 1 4 2 4 3 1 4 5 4 2 0 3 1 0 3\n",
            " 3 2 4 4 5 2 0 1 2 0 3 2 5 2 5 2 1 0 3 4 0 5 2 0 2 1 2 2 0 2 3 5 4 1 5 5 5\n",
            " 4 4 4 0 3 4 3 5 2 4 1 5 0 4 2 0 3 1 3 4 5 3 3 3 2 0 0 2 5 3 2 1 1 2 3 3 0\n",
            " 3 0 5 4 1 2 1 4 4 4 2 3 4 3 5 1 3 0 4]\n",
            "Policy is converged at 17-th iteration.\n",
            "Policy from Policy Iteration:  [4 4 4 4 0 0 0 0 0 0 0 0 0 0 0 0 5 0 0 0 3 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 3\n",
            " 0 0 0 0 0 0 0 2 2 2 2 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 2 2 2 0 0 0 0 0 0\n",
            " 0 0 0 2 0 0 0 0 0 0 4 4 4 4 0 0 0 0 0 0 0 0 0 5 0 0 1 1 1 1 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1\n",
            " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1\n",
            " 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 2 2 2 2 0 0 0 0 2 2 2 2 1 2 0 2 1 1\n",
            " 1 1 2 2 2 2 3 3 3 3 2 2 2 2 1 2 3 2 3 3 3 3 1 1 1 1 3 3 3 3 2 2 2 2 3 1 3\n",
            " 2 3 3 3 3 1 1 1 1 3 3 3 3 0 0 0 0 3 1 3 0 3 3 3 3 1 1 1 1 3 3 3 3 0 0 0 0\n",
            " 3 1 3 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1\n",
            " 1 4 4 4 4 1 1 1 1 1 1 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 1 1 1 5 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 1 1 1 3] \n",
            "\n",
            "Number of successes of Policy Iteration in map Taxi-v3 : 2000000/2000000, Time to find policy: 1.03981614112854, Total time: 251.15985131263733s, Average time per trial: 0.00012557992565631867s, Average steps: 13.070936 steps\n",
            "Number of successes of Value Iteration in map Taxi-v3 : 2000000/2000000, Time to find policy: 0.5977301597595215, Total time: 255.17877459526062s, Average time per trial: 0.0001275893872976303s, Average steps: 13.068731 steps\n",
            "\n",
            "\n",
            "Trial number  4\n",
            "Converged at 116-th iteration.\n",
            "Policy from Value Iteration:  [4 4 4 4 0 0 0 0 0 0 0 0 0 0 0 0 5 0 0 0 3 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 3\n",
            " 0 0 0 0 0 0 0 2 2 2 2 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 2 2 2 0 0 0 0 0 0\n",
            " 0 0 0 2 0 0 0 0 0 0 4 4 4 4 0 0 0 0 0 0 0 0 0 5 0 0 1 1 1 1 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1\n",
            " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1\n",
            " 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 2 2 2 2 0 0 0 0 2 2 2 2 1 2 0 2 1 1\n",
            " 1 1 2 2 2 2 3 3 3 3 2 2 2 2 1 2 3 2 3 3 3 3 1 1 1 1 3 3 3 3 2 2 2 2 3 1 3\n",
            " 2 3 3 3 3 1 1 1 1 3 3 3 3 0 0 0 0 3 1 3 0 3 3 3 3 1 1 1 1 3 3 3 3 0 0 0 0\n",
            " 3 1 3 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1\n",
            " 1 4 4 4 4 1 1 1 1 1 1 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 1 1 1 5 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 1 1 1 3] \n",
            "\n",
            "Initialize a random policy: [3 5 4 5 4 1 2 3 4 0 2 1 5 1 1 4 5 3 3 0 0 4 3 3 4 5 0 0 4 0 0 0 1 3 3 4 1\n",
            " 5 4 1 1 3 5 4 3 1 0 5 3 2 5 1 4 5 1 2 4 4 2 4 5 1 4 5 3 0 3 3 0 5 3 4 5 5\n",
            " 1 5 0 5 5 5 0 0 5 1 4 0 2 2 2 3 4 5 4 0 0 1 2 2 0 5 0 2 4 5 5 0 2 2 5 2 1\n",
            " 1 1 0 0 5 5 4 2 4 1 3 1 3 4 4 2 3 3 0 5 0 4 4 1 1 2 4 2 5 5 5 3 2 3 2 0 2\n",
            " 5 1 4 4 3 5 5 3 4 1 3 1 5 2 3 1 1 0 4 2 1 0 0 4 4 4 3 2 5 0 5 5 0 5 5 5 0\n",
            " 2 2 2 3 2 2 3 0 4 0 1 3 5 1 3 3 1 2 1 4 3 1 5 1 5 1 2 3 4 0 4 2 2 3 4 4 5\n",
            " 3 4 1 4 1 5 5 2 1 3 4 1 5 4 2 3 4 1 5 2 5 0 4 0 2 0 1 4 2 3 1 1 2 3 4 2 4\n",
            " 1 5 0 5 2 2 0 1 2 3 2 3 1 0 1 3 5 4 3 3 1 0 2 0 1 5 3 2 0 2 4 2 1 0 3 0 3\n",
            " 3 3 1 0 3 3 4 2 1 3 0 1 4 1 3 0 2 1 4 0 1 3 2 2 2 4 4 5 5 4 3 1 2 1 4 4 2\n",
            " 1 0 5 4 3 0 5 0 5 3 5 3 4 0 2 2 5 1 0 0 5 4 4 3 4 5 3 1 1 2 4 4 5 2 2 2 4\n",
            " 3 4 1 4 1 3 4 1 1 2 5 3 2 0 1 2 4 1 5 1 3 0 0 2 5 0 0 1 2 0 3 0 1 0 4 2 0\n",
            " 1 2 2 2 0 1 0 5 4 0 5 2 2 0 5 4 3 0 4 3 0 1 5 4 4 3 1 4 3 1 1 0 2 5 0 2 4\n",
            " 0 4 0 2 5 1 5 2 3 5 1 5 5 5 5 4 2 4 0 0 4 4 1 3 4 4 0 1 0 1 0 0 3 3 0 4 3\n",
            " 0 4 4 4 4 0 2 2 0 4 3 1 2 5 4 0 0 4 4]\n",
            "Policy is converged at 16-th iteration.\n",
            "Policy from Policy Iteration:  [4 4 4 4 0 0 0 0 0 0 0 0 0 0 0 0 5 0 0 0 3 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 3\n",
            " 0 0 0 0 0 0 0 2 2 2 2 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 2 2 2 0 0 0 0 0 0\n",
            " 0 0 0 2 0 0 0 0 0 0 4 4 4 4 0 0 0 0 0 0 0 0 0 5 0 0 1 1 1 1 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1\n",
            " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1\n",
            " 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 2 2 2 2 0 0 0 0 2 2 2 2 1 2 0 2 1 1\n",
            " 1 1 2 2 2 2 3 3 3 3 2 2 2 2 1 2 3 2 3 3 3 3 1 1 1 1 3 3 3 3 2 2 2 2 3 1 3\n",
            " 2 3 3 3 3 1 1 1 1 3 3 3 3 0 0 0 0 3 1 3 0 3 3 3 3 1 1 1 1 3 3 3 3 0 0 0 0\n",
            " 3 1 3 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1\n",
            " 1 4 4 4 4 1 1 1 1 1 1 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 1 1 1 5 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 1 1 1 3] \n",
            "\n",
            "Number of successes of Policy Iteration in map Taxi-v3 : 2000000/2000000, Time to find policy: 0.9992475509643555, Total time: 254.01556491851807s, Average time per trial: 0.00012700778245925903s, Average steps: 13.0643825 steps\n",
            "Number of successes of Value Iteration in map Taxi-v3 : 2000000/2000000, Time to find policy: 0.5859925746917725, Total time: 257.4667749404907s, Average time per trial: 0.00012873338747024536s, Average steps: 13.0694425 steps\n",
            "\n",
            "\n",
            "Trial number  5\n",
            "Converged at 116-th iteration.\n",
            "Policy from Value Iteration:  [4 4 4 4 0 0 0 0 0 0 0 0 0 0 0 0 5 0 0 0 3 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 3\n",
            " 0 0 0 0 0 0 0 2 2 2 2 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 2 2 2 0 0 0 0 0 0\n",
            " 0 0 0 2 0 0 0 0 0 0 4 4 4 4 0 0 0 0 0 0 0 0 0 5 0 0 1 1 1 1 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1\n",
            " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1\n",
            " 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 2 2 2 2 0 0 0 0 2 2 2 2 1 2 0 2 1 1\n",
            " 1 1 2 2 2 2 3 3 3 3 2 2 2 2 1 2 3 2 3 3 3 3 1 1 1 1 3 3 3 3 2 2 2 2 3 1 3\n",
            " 2 3 3 3 3 1 1 1 1 3 3 3 3 0 0 0 0 3 1 3 0 3 3 3 3 1 1 1 1 3 3 3 3 0 0 0 0\n",
            " 3 1 3 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1\n",
            " 1 4 4 4 4 1 1 1 1 1 1 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 1 1 1 5 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 1 1 1 3] \n",
            "\n",
            "Initialize a random policy: [4 1 0 4 5 1 5 3 3 1 2 2 4 1 2 5 4 2 2 4 0 1 4 5 0 1 1 5 0 3 2 0 0 0 5 5 0\n",
            " 0 0 0 2 0 3 0 2 2 2 3 5 5 2 2 1 5 1 5 0 3 5 3 0 3 0 5 2 1 1 0 1 4 2 0 2 0\n",
            " 5 5 4 5 1 3 3 1 4 0 4 2 1 1 5 3 0 1 4 1 4 1 2 0 2 4 1 0 5 5 5 1 2 2 2 5 2\n",
            " 5 1 0 5 1 1 0 0 0 0 4 4 5 4 3 5 1 1 3 4 5 4 2 4 2 0 5 2 0 0 4 3 3 1 2 3 0\n",
            " 2 0 3 1 3 1 3 2 5 1 5 1 1 3 2 1 2 2 5 0 5 4 2 3 4 4 4 3 4 1 0 5 2 5 2 4 1\n",
            " 3 3 1 0 3 1 1 3 0 3 0 3 3 5 4 4 5 0 0 2 0 2 4 0 2 0 0 2 3 1 3 3 1 0 0 2 2\n",
            " 3 0 0 5 3 1 0 5 2 2 2 5 4 4 2 0 2 5 4 5 1 1 3 1 5 5 2 0 4 5 1 3 1 5 5 5 5\n",
            " 1 2 5 5 5 4 1 4 0 1 1 2 0 1 1 5 2 4 4 3 1 3 0 0 4 2 5 0 1 2 4 2 0 0 1 3 5\n",
            " 2 3 2 0 5 1 2 0 2 3 3 2 1 3 4 3 2 1 0 3 1 1 1 1 4 2 5 5 1 4 3 4 2 5 0 3 5\n",
            " 4 1 0 3 5 4 1 4 1 2 5 1 0 0 5 4 4 4 4 4 2 4 1 3 3 5 5 3 1 4 5 4 0 2 1 1 1\n",
            " 2 4 2 2 1 3 3 5 3 0 1 2 5 5 5 3 4 4 0 5 0 4 4 0 1 3 1 2 0 4 2 5 2 5 2 0 3\n",
            " 5 2 4 5 5 2 2 3 3 4 1 3 4 4 2 4 3 0 5 5 2 0 0 0 1 2 2 2 5 5 1 4 3 5 1 1 3\n",
            " 3 2 0 5 3 2 5 1 4 4 4 3 0 3 1 2 5 4 4 2 1 5 1 5 4 0 5 2 3 5 4 1 4 4 2 0 1\n",
            " 2 4 5 3 0 2 2 3 1 3 2 3 4 3 2 4 4 4 4]\n",
            "Policy is converged at 16-th iteration.\n",
            "Policy from Policy Iteration:  [4 4 4 4 0 0 0 0 0 0 0 0 0 0 0 0 5 0 0 0 3 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 3\n",
            " 0 0 0 0 0 0 0 2 2 2 2 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 2 2 2 0 0 0 0 0 0\n",
            " 0 0 0 2 0 0 0 0 0 0 4 4 4 4 0 0 0 0 0 0 0 0 0 5 0 0 1 1 1 1 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1\n",
            " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1\n",
            " 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 2 2 2 2 0 0 0 0 2 2 2 2 1 2 0 2 1 1\n",
            " 1 1 2 2 2 2 3 3 3 3 2 2 2 2 1 2 3 2 3 3 3 3 1 1 1 1 3 3 3 3 2 2 2 2 3 1 3\n",
            " 2 3 3 3 3 1 1 1 1 3 3 3 3 0 0 0 0 3 1 3 0 3 3 3 3 1 1 1 1 3 3 3 3 0 0 0 0\n",
            " 3 1 3 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1\n",
            " 1 4 4 4 4 1 1 1 1 1 1 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 1 1 1 5 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 1 1 1 3] \n",
            "\n",
            "Number of successes of Policy Iteration in map Taxi-v3 : 2000000/2000000, Time to find policy: 1.0259571075439453, Total time: 254.9631745815277s, Average time per trial: 0.00012748158729076385s, Average steps: 13.070547 steps\n",
            "Number of successes of Value Iteration in map Taxi-v3 : 2000000/2000000, Time to find policy: 0.5798523426055908, Total time: 254.62425565719604s, Average time per trial: 0.00012731212782859802s, Average steps: 13.0676785 steps\n",
            "\n",
            "\n",
            "\n",
            "Mean number of successes for Value Iteration: 2000000.0/2000000\n",
            "Mean time to find policy for Value Iteration: 0.5777020454406738s\n",
            "Mean total time for Value Iteration: 253.55959005355834s\n",
            "Mean average time per trial for Value Iteration: 0.00012677979502677918s\n",
            "Mean average steps for Value Iteration: 13.0690033 steps\n",
            "\n",
            "Mean number of successes for Policy Iteration: 2000000.0/2000000\n",
            "Mean time to find policy for Policy Iteration: 0.9822583675384522s\n",
            "Mean total time for Policy Iteration: 253.50532903671265s\n",
            "Mean average time per trial for Policy Iteration: 0.0001267526645183563s\n",
            "Mean average steps for Policy Iteration: 13.069405799999998 steps\n"
          ]
        }
      ],
      "source": [
        "successes_vi = []\n",
        "times_to_find_policy_vi = []\n",
        "total_times_vi = []\n",
        "average_times_per_trial_vi = []\n",
        "average_steps_vi = []\n",
        "\n",
        "successes_pi = []\n",
        "times_to_find_policy_pi = []\n",
        "total_times_pi = []\n",
        "average_times_per_trial_pi = []\n",
        "average_steps_pi = []\n",
        "\n",
        "for i in range(5):\n",
        "    print(\"Trial number \", i + 1)\n",
        "    time_find_optimal_policy_vi_start = time.time()\n",
        "    vi_value = value_iteration(env_3, 500, 0.9)\n",
        "    policy_from_vi_value = policy_extraction(env_3, vi_value, 0.9)\n",
        "    time_find_optimal_policy_vi_end = time.time() - time_find_optimal_policy_vi_start\n",
        "    print(\"Policy from Value Iteration: \", policy_from_vi_value, \"\\n\")\n",
        "    start_vi = time.time()\n",
        "    vi_number_of_successes, vi_mean_of_steps = play_multiple_times(env_3, policy_from_vi_value, num_trials_for_Taxi)\n",
        "    vi_time = time.time() - start_vi\n",
        "    time_find_optimal_policy_pi_start = time.time()\n",
        "    policy_pi = policy_iteration(env_3, 500, 0.9)\n",
        "    time_find_optimal_policy_pi_end = time.time() - time_find_optimal_policy_pi_start\n",
        "    print(\"Policy from Policy Iteration: \", policy_pi, \"\\n\")\n",
        "    start_pi = time.time()\n",
        "    pi_number_of_successes, pi_mean_of_steps = play_multiple_times(env_3, policy_pi, num_trials_for_Taxi)\n",
        "    pi_time = time.time() - start_pi\n",
        "    print(f'Number of successes of Policy Iteration in map Taxi-v3 : {pi_number_of_successes}/{num_trials_for_Taxi}, Time to find policy: {time_find_optimal_policy_pi_end}, Total time: {pi_time}s, Average time per trial: {pi_time/num_trials_for_Taxi}s, Average steps: {pi_mean_of_steps} steps')\n",
        "    print(f'Number of successes of Value Iteration in map Taxi-v3 : {vi_number_of_successes}/{num_trials_for_Taxi}, Time to find policy: {time_find_optimal_policy_vi_end}, Total time: {vi_time}s, Average time per trial: {vi_time/num_trials_for_Taxi}s, Average steps: {vi_mean_of_steps} steps')\n",
        "    print(\"\\n\")\n",
        "    successes_vi.append(vi_number_of_successes)\n",
        "    times_to_find_policy_vi.append(time_find_optimal_policy_vi_end)\n",
        "    total_times_vi.append(vi_time)\n",
        "    average_times_per_trial_vi.append(vi_time / num_trials_for_Taxi)\n",
        "    average_steps_vi.append(vi_mean_of_steps)\n",
        "    successes_pi.append(pi_number_of_successes)\n",
        "    times_to_find_policy_pi.append(time_find_optimal_policy_pi_end)\n",
        "    total_times_pi.append(pi_time)\n",
        "    average_times_per_trial_pi.append(pi_time / num_trials_for_Taxi)\n",
        "    average_steps_pi.append(pi_mean_of_steps)\n",
        "\n",
        "mean_successes_vi = np.mean(successes_vi)\n",
        "mean_time_to_find_policy_vi = np.mean(times_to_find_policy_vi)\n",
        "mean_total_time_vi = np.mean(total_times_vi)\n",
        "mean_average_time_per_trial_vi = np.mean(average_times_per_trial_vi)\n",
        "mean_average_steps_vi = np.mean(average_steps_vi)\n",
        "\n",
        "mean_successes_pi = np.mean(successes_pi)\n",
        "mean_time_to_find_policy_pi = np.mean(times_to_find_policy_pi)\n",
        "mean_total_time_pi = np.mean(total_times_pi)\n",
        "mean_average_time_per_trial_pi = np.mean(average_times_per_trial_pi)\n",
        "mean_average_steps_pi = np.mean(average_steps_pi)\n",
        "\n",
        "print(f'\\nMean number of successes for Value Iteration: {mean_successes_vi}/{num_trials_for_Taxi}')\n",
        "print(f'Mean time to find policy for Value Iteration: {mean_time_to_find_policy_vi}s')\n",
        "print(f'Mean total time for Value Iteration: {mean_total_time_vi}s')\n",
        "print(f'Mean average time per trial for Value Iteration: {mean_average_time_per_trial_vi}s')\n",
        "print(f'Mean average steps for Value Iteration: {mean_average_steps_vi} steps')\n",
        "\n",
        "print(f'\\nMean number of successes for Policy Iteration: {mean_successes_pi}/{num_trials_for_Taxi}')\n",
        "print(f'Mean time to find policy for Policy Iteration: {mean_time_to_find_policy_pi}s')\n",
        "print(f'Mean total time for Policy Iteration: {mean_total_time_pi}s')\n",
        "print(f'Mean average time per trial for Policy Iteration: {mean_average_time_per_trial_pi}s')\n",
        "print(f'Mean average steps for Policy Iteration: {mean_average_steps_pi} steps')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Convergence Speed:** \n",
        "- Policy Iteration typically converges faster in the Frozen Lake environment but may take longer in the Taxi environment.\n",
        "\n",
        "**Success Rate:** \n",
        "- Both methods have similar success rates.\n",
        "\n",
        "**Time Efficiency:** \n",
        "- In the Frozen Lake and Frozen Lake 8x8 environment, Policy Iteration generally takes less time compared to Value Iteration. However, in the Taxi environment, Value Iteration may be more efficient.\n",
        "\n",
        "**Steps:** \n",
        "- The average number of steps to complete the game is usually similar for both methods.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
